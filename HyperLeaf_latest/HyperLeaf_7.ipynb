{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79296312",
   "metadata": {},
   "source": [
    "Section 1(Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0baaa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max Normalization for Hyperspectral Data\n",
    "import pandas as pd                       # For reading/writing Excel/CSV and handling DataFrames\n",
    "from sklearn.preprocessing import MinMaxScaler   # For Min-Max scaling of features\n",
    "\n",
    "#  Load the dataset \n",
    "input_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/full_dataset_filtered.xlsx\"  # Path to Excel file\n",
    "df = pd.read_excel(input_path)            # Read the Excel file into a DataFrame\n",
    "\n",
    "print(\"Original Columns:\", df.columns.tolist())   # Show all column names for verification\n",
    "print(\"Shape before:\", df.shape)                  # Print dataset shape (rows, columns) before preprocessing\n",
    "\n",
    "#  Drop unwanted columns \n",
    "drop_cols = ['Image ID', 'Minimum Reflectance', 'Otsu Threshold']  # Columns to remove\n",
    "df = df.drop(columns=drop_cols, errors='ignore')  # Drop them; ignore errors if any column not present\n",
    "\n",
    "#  Separate target column \n",
    "target_col = 'Yield'                      # This is the target variable\n",
    "y = df[target_col]                         # Store Yield separately (we don’t normalize it)\n",
    "X = df.drop(columns=[target_col])          # All other columns are features to normalize\n",
    "\n",
    "#  Min-Max normalization \n",
    "scaler = MinMaxScaler()                    # Create MinMaxScaler instance (default range: 0–1)\n",
    "X_scaled = scaler.fit_transform(X)         # Fit scaler to features & transform them into scaled array\n",
    "\n",
    "#  Reconstruct DataFrame \n",
    "normalized_df = pd.DataFrame(X_scaled, columns=X.columns)  # Convert scaled array back to DataFrame with original column names\n",
    "normalized_df[target_col] = y                               # Add back the original Yield column (unscaled)\n",
    "normalized_df = normalized_df.round(2) \n",
    "print(\"Shape after normalization:\", normalized_df.shape)    # Print new shape after dropping columns and adding scaled features\n",
    "\n",
    "#  Save processed file \n",
    "output_path = \"normalized_minmax_with_target_2.csv\"           # Output file name (will save in current working directory)\n",
    "normalized_df.to_csv(output_path, index=False)              # Save normalized data to CSV without row index\n",
    "\n",
    "print(f\"✅ Normalized file saved as: {output_path}\")         # Confirmation message after saving\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d4eb68",
   "metadata": {},
   "source": [
    "Section 2(KS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8b1b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data partition using KS (Kennard–Stone) method\n",
    "import pandas as pd                       # For handling CSV files and tabular data\n",
    "import numpy as np                        # For numerical operations\n",
    "from scipy.spatial.distance import cdist  # To compute pairwise Euclidean distances\n",
    "\n",
    "# Load dataset from CSV file\n",
    "input_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/normalized_minmax_with_target_2.csv\"\n",
    "df = pd.read_csv(input_path)              # Read CSV into pandas DataFrame\n",
    "\n",
    "# Identify columns to keep (not for splitting)\n",
    "keep_cols = ['Image ID', 'Yield', 'Minimum Reflectance', 'Otsu'] # Keep only the columns that actually exist in the dataset \n",
    "\n",
    "keep_cols_existing = [col for col in keep_cols if col in df.columns]  # Select remaining columns (hyperspectral bands) for splitting\n",
    "\n",
    "X_cols = [col for col in df.columns if col not in keep_cols_existing]  \n",
    "X = df[X_cols].values                     # Extract band values as NumPy array for KS algorithm\n",
    "\n",
    "# Define Kennard–Stone algorithm function\n",
    "def kennard_stone(X, n_samples):\n",
    "    n_total = X.shape[0]                              # Total number of samples\n",
    "    dist_matrix = cdist(X, X, metric='euclidean')     # Compute pairwise Euclidean distance matrix\n",
    "    \n",
    "    # Step 1: select two samples farthest apart\n",
    "    i1, i2 = np.unravel_index(np.argmax(dist_matrix, axis=None), dist_matrix.shape)  \n",
    "    selected = [i1, i2]                               # Start with two farthest points\n",
    "    remaining = list(set(range(n_total)) - set(selected))  # Remaining samples to select from\n",
    "    \n",
    "    # Step 2: iteratively select sample farthest from selected set\n",
    "    while len(selected) < n_samples:\n",
    "        # Compute minimum distance of each remaining sample to any selected sample\n",
    "        dist_to_selected = np.min(dist_matrix[remaining][:, selected], axis=1)  \n",
    "        next_idx = remaining[np.argmax(dist_to_selected)]  # Pick the one with maximum distance\n",
    "        selected.append(next_idx)                           # Add to selected list\n",
    "        remaining.remove(next_idx)                          # Remove from remaining\n",
    "    \n",
    "    return selected  # Return list of selected indices for training set\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "n_train = int(0.8 * X.shape[0])   # 80% of samples for training\n",
    "train_idx = kennard_stone(X, n_train)  # Get training indices using KS algorithm\n",
    "test_idx = list(set(range(X.shape[0])) - set(train_idx))  # Remaining samples for testing\n",
    "\n",
    "# Create DataFrames for train and test sets\n",
    "df_train = df.iloc[train_idx].reset_index(drop=True)  # Training set\n",
    "df_test = df.iloc[test_idx].reset_index(drop=True)    # Test set\n",
    "\n",
    "# Save train and test sets to CSV files\n",
    "df_train.to_csv(r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/train_ks_2.csv\", index=False)\n",
    "df_test.to_csv(r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/test_ks_2.csv\", index=False)\n",
    "\n",
    "# Print confirmation and dataset shapes\n",
    "print(\"✅ Kennard–Stone split done!\")  \n",
    "print(\"Train set shape:\", df_train.shape)\n",
    "print(\"Test set shape :\", df_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1bcf17",
   "metadata": {},
   "source": [
    "Section 3(PLSR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9c8405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using PLSR (Partial Least Squares Regression) technique\n",
    "import pandas as pd  # For data handling\n",
    "import numpy as np  # For numerical operations\n",
    "from sklearn.cross_decomposition import PLSRegression  # PLS regression model\n",
    "from sklearn.preprocessing import StandardScaler  # Feature scaling\n",
    "from sklearn.metrics import mean_squared_error, r2_score  # Evaluation metrics\n",
    "\n",
    "# Load train and test datasets\n",
    "train_df = pd.read_csv(r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/train_ks_2.csv\")  # Load training CSV\n",
    "test_df = pd.read_csv(r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/test_ks_2.csv\")  # Load testing CSV\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)  # Show training data shape\n",
    "print(\"Test shape:\", test_df.shape)  # Show testing data shape\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "target_column = \"Yield\"  # Replace with your actual target column name\n",
    "X_train = train_df.drop(columns=[target_column])  # Features for training\n",
    "y_train = train_df[target_column]  # Target for training\n",
    "\n",
    "X_test = test_df.drop(columns=[target_column])  # Features for testing\n",
    "y_test = test_df[target_column]  # Target for testing\n",
    "\n",
    "# Standardize the feature data\n",
    "scaler = StandardScaler()  # Initialize scaler\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit and transform training features\n",
    "X_test_scaled = scaler.transform(X_test)  # Transform testing features with same scaler\n",
    "\n",
    "# Train PLSR model with up to 10 components\n",
    "n_components = min(X_train.shape[1], 50)  # Choose number of components (max 10 or number of features)\n",
    "pls = PLSRegression(n_components=n_components)  # Initialize PLS model\n",
    "pls.fit(X_train_scaled, y_train)  # Fit PLS model to training data\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = pls.predict(X_train_scaled)  # Predict on training set\n",
    "y_pred_test = pls.predict(X_test_scaled)  # Predict on testing set\n",
    "\n",
    "# Evaluate model performance\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))  # RMSE for training\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))  # RMSE for testing\n",
    "\n",
    "train_r2 = r2_score(y_train, y_pred_train)  # R² for training\n",
    "test_r2 = r2_score(y_test, y_pred_test)  # R² for testing\n",
    "\n",
    "train_rpd = np.std(y_train, ddof=1) / train_rmse  # RPD for training\n",
    "test_rpd = np.std(y_test, ddof=1) / test_rmse  # RPD for testing\n",
    "\n",
    "print(\"\\nPLSR Model Evaluation:\")  # Print evaluation header\n",
    "print(f\"Train R²             : {train_r2:.4f}\")  # Display training R²\n",
    "print(f\"Test R²              : {test_r2:.4f}\")  # Display testing R²\n",
    "print(f\"Train RMSE           : {train_rmse:.4f}\")  # Display training RMSE\n",
    "print(f\"Test RMSE            : {test_rmse:.4f}\")  # Display testing RMSE\n",
    "print(f\"Train RPD            : {train_rpd:.4f}\")  # Display training RPD\n",
    "print(f\"Test RPD             : {test_rpd:.4f}\")  # Display testing RPD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2f428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial Least Squares Regression (PLSR) + RandomizedSearchCV\n",
    "import pandas as pd  # For handling CSV data\n",
    "import numpy as np  # For numerical computations\n",
    "from sklearn.preprocessing import StandardScaler  # For feature scaling\n",
    "from sklearn.cross_decomposition import PLSRegression  # PLSR model\n",
    "from sklearn.model_selection import RandomizedSearchCV  # Randomized hyperparameter tuning\n",
    "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, ConfusionMatrixDisplay  # Evaluation metrics\n",
    "import matplotlib.pyplot as plt  # For visualization\n",
    "import pickle  # For saving model and scaler\n",
    "\n",
    "# 1. Load train and test datasets\n",
    "train_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/train_ks_2.csv\"\n",
    "test_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/test_ks_2.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# 2. Split features (X) and target (Y)\n",
    "target_column = \"Yield\"  # Dependent variable\n",
    "X_train = train_df.drop(columns=[target_column]).values\n",
    "y_train = train_df[target_column].values\n",
    "X_test = test_df.drop(columns=[target_column]).values\n",
    "y_test = test_df[target_column].values\n",
    "\n",
    "# 3. Standardize features for PLSR\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "# Save the scaler for later use\n",
    "with open(\"scaler_X.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler_X, f)\n",
    "\n",
    "# 4. Define PLSR model\n",
    "pls = PLSRegression(scale=False)  # Already scaled data\n",
    "\n",
    "# 5. Define parameter search space for RandomizedSearchCV\n",
    "param_distributions = {\n",
    "    'n_components': np.arange(2, min(X_train.shape[1], 12))  # Randomly choose number of latent variables (2–30)\n",
    "}\n",
    "\n",
    "# 6. Set up RandomizedSearchCV for faster tuning\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pls,               # PLSR model\n",
    "    param_distributions=param_distributions,  # Search range\n",
    "    n_iter=10,                   # Number of random combinations to test\n",
    "    scoring='r2',                # Evaluation metric\n",
    "    cv=10,                       # 10-fold cross-validation\n",
    "    random_state=42,             # Reproducibility\n",
    "    n_jobs=-1,                   # Use all CPU cores\n",
    "    verbose=2                    # Show progress\n",
    ")\n",
    "\n",
    "print(\"\\n Starting RandomizedSearchCV for PLSR...\")\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 7. Display the best result\n",
    "print(\"\\n Best Parameters found:\")\n",
    "print(random_search.best_params_)\n",
    "print(\" Best Cross-Validation R²:\", random_search.best_score_)\n",
    "\n",
    "# Get the best model\n",
    "best_pls = random_search.best_estimator_\n",
    "\n",
    "# 8. Save best model\n",
    "with open(\"PLSR_model_best.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_pls, f)\n",
    "print(\"\\n Best PLSR model saved as PLSR_model_best.pkl\")\n",
    "\n",
    "# 9. Predictions on train and test sets\n",
    "y_pred_train = best_pls.predict(X_train_scaled).ravel()\n",
    "y_pred_test = best_pls.predict(X_test_scaled).ravel()\n",
    "\n",
    "# 10. Define evaluation function (R², RMSE, RPD)\n",
    "def evaluate(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rpd = np.std(y_true, ddof=1) / rmse\n",
    "    return r2, rmse, rpd\n",
    "\n",
    "# Compute metrics\n",
    "train_r2, train_rmse, train_rpd = evaluate(y_train, y_pred_train)\n",
    "test_r2, test_rmse, test_rpd = evaluate(y_test, y_pred_test)\n",
    "\n",
    "# 11. Print evaluation summary\n",
    "print(\"\\n PLSR Model Evaluation (RandomizedSearchCV):\")\n",
    "print(f\"Train R²   : {train_r2:.4f}\")\n",
    "print(f\"Test R²    : {test_r2:.4f}\")\n",
    "print(f\"Train RMSE : {train_rmse:.4f}\")\n",
    "print(f\"Test RMSE  : {test_rmse:.4f}\")\n",
    "print(f\"Train RPD  : {train_rpd:.4f}\")\n",
    "print(f\"Test RPD   : {test_rpd:.4f}\")\n",
    "\n",
    "# 12. Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# (a) Train Predicted vs Actual\n",
    "axes[0, 0].scatter(y_train, y_pred_train, color='blue', alpha=0.6, label='Train Predicted')\n",
    "z_train = np.polyfit(y_train, y_pred_train, 1)\n",
    "p_train = np.poly1d(z_train)\n",
    "axes[0, 0].plot(y_train, p_train(y_train), \"blue\", linestyle='--', label=\"Train Best Fit\")\n",
    "axes[0, 0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \"green\", linestyle='-', label=\"Ideal Fit\")\n",
    "axes[0, 0].set_title(\"Training Set: Predicted vs Actual\")\n",
    "axes[0, 0].set_xlabel(\"Actual Values\")\n",
    "axes[0, 0].set_ylabel(\"Predicted Values\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# (b) Test Predicted vs Actual\n",
    "axes[0, 1].scatter(y_test, y_pred_test, color='red', alpha=0.6, label='Test Predicted')\n",
    "z_test = np.polyfit(y_test, y_pred_test, 1)\n",
    "p_test = np.poly1d(z_test)\n",
    "axes[0, 1].plot(y_test, p_test(y_test), \"orange\", linestyle='--', label=\"Test Best Fit\")\n",
    "axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"green\", linestyle='-', label=\"Ideal Fit\")\n",
    "axes[0, 1].set_title(\"Test Set: Predicted vs Actual\")\n",
    "axes[0, 1].set_xlabel(\"Actual Values\")\n",
    "axes[0, 1].set_ylabel(\"Predicted Values\")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# (c) Feature Importance (LV1 Weights)\n",
    "axes[1, 0].bar(range(X_train.shape[1]), np.abs(best_pls.x_weights_[:, 0]), color='purple')\n",
    "axes[1, 0].set_title(\"PLSR Variable Importance (LV1 Weights)\")\n",
    "axes[1, 0].set_xlabel(\"Feature Index\")\n",
    "axes[1, 0].set_ylabel(\"Weight Magnitude\")\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# (d) Confusion Matrix (High vs Low Yield classification-like visualization)\n",
    "y_test_class = (y_test >= np.median(y_test)).astype(int)\n",
    "y_pred_test_class = (y_pred_test >= np.median(y_test)).astype(int)\n",
    "cm = confusion_matrix(y_test_class, y_pred_test_class)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Low Yield\", \"High Yield\"])\n",
    "disp.plot(ax=axes[1, 1], cmap=plt.cm.Blues, values_format=\"d\")\n",
    "axes[1, 1].set_title(\"Confusion Matrix\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820fa03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial Least Squares Regression (PLSR) + GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# 1. Load CSV files\n",
    "train_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/train_ks_2.csv\"\n",
    "test_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/test_ks_2.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# 2. Separate features and target\n",
    "target_column = \"Yield\"\n",
    "X_train = train_df.drop(columns=[target_column]).values\n",
    "y_train = train_df[target_column].values\n",
    "X_test = test_df.drop(columns=[target_column]).values\n",
    "y_test = test_df[target_column].values\n",
    "\n",
    "# 3. Standardize only X\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "with open(\"scaler_X.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler_X, f)\n",
    "\n",
    "# 4. Hyperparameter tuning: number of latent variables\n",
    "param_grid = {\n",
    "    'n_components': list(range(2, min(X_train_scaled.shape[1], 12)))  # test up to 25 LVs if possible\n",
    "}\n",
    "\n",
    "pls = PLSRegression(scale=False)\n",
    "\n",
    "cv_strategy = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pls,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',  # better for minimizing RMSECV\n",
    "    cv=cv_strategy,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"\\n Starting GridSearchCV for PLSR...\")\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best parameters\n",
    "best_pls = grid_search.best_estimator_\n",
    "best_ncomp = grid_search.best_params_['n_components']\n",
    "print(f\"\\n Best Parameters found: {grid_search.best_params_}\")\n",
    "print(f\" Best CV (lowest MSE): {-grid_search.best_score_:.4f}\")\n",
    "print(f\" Selected LVs: {best_ncomp}\")\n",
    "\n",
    "# Save model\n",
    "with open(\"PLSR_model_best.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_pls, f)\n",
    "print(\"\\n Best PLSR model saved as PLSR_model_best.pkl\")\n",
    "\n",
    "# 5. Predictions\n",
    "y_pred_train = best_pls.predict(X_train_scaled).ravel()\n",
    "y_pred_test = best_pls.predict(X_test_scaled).ravel()\n",
    "\n",
    "# 6. Evaluation\n",
    "def evaluate(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rpd = np.std(y_true, ddof=1) / rmse\n",
    "    return r2, rmse, rpd\n",
    "\n",
    "train_r2, train_rmse, train_rpd = evaluate(y_train, y_pred_train)\n",
    "test_r2, test_rmse, test_rpd = evaluate(y_test, y_pred_test)\n",
    "\n",
    "print(\"\\nPLSR Model Evaluation:\")\n",
    "print(f\"Train R²   : {train_r2:.4f}\")\n",
    "print(f\"Test R²    : {test_r2:.4f}\")\n",
    "print(f\"Train RMSE : {train_rmse:.4f}\")\n",
    "print(f\"Test RMSE  : {test_rmse:.4f}\")\n",
    "print(f\"Train RPD  : {train_rpd:.4f}\")\n",
    "print(f\"Test RPD   : {test_rpd:.4f}\")\n",
    "\n",
    "# 7. Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Train Scatter Plot\n",
    "axes[0, 0].scatter(y_train, y_pred_train, color='blue', alpha=0.6, label='Train Predicted')\n",
    "axes[0, 0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()],\n",
    "                \"green\", linestyle='-', label=\"Ideal Fit\")\n",
    "axes[0, 0].set_title(\"Training Set: Predicted vs Actual\")\n",
    "axes[0, 0].set_xlabel(\"Actual Values\")\n",
    "axes[0, 0].set_ylabel(\"Predicted Values\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Test Scatter Plot\n",
    "axes[0, 1].scatter(y_test, y_pred_test, color='red', alpha=0.6, label='Test Predicted')\n",
    "axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],\n",
    "                \"green\", linestyle='-', label=\"Ideal Fit\")\n",
    "axes[0, 1].set_title(\"Test Set: Predicted vs Actual\")\n",
    "axes[0, 1].set_xlabel(\"Actual Values\")\n",
    "axes[0, 1].set_ylabel(\"Predicted Values\")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Feature Importance (average absolute weights)\n",
    "avg_weights = np.mean(np.abs(best_pls.x_weights_), axis=1)\n",
    "axes[1, 0].bar(range(X_train.shape[1]), avg_weights, color='purple')\n",
    "axes[1, 0].set_title(\"PLSR Variable Importance (Mean Absolute Weights)\")\n",
    "axes[1, 0].set_xlabel(\"Feature Index\")\n",
    "axes[1, 0].set_ylabel(\"Weight Magnitude\")\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Confusion Matrix\n",
    "y_test_class = (y_test >= np.median(y_test)).astype(int)\n",
    "y_pred_test_class = (y_pred_test >= np.median(y_test)).astype(int)\n",
    "cm = confusion_matrix(y_test_class, y_pred_test_class)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Low Yield\", \"High Yield\"])\n",
    "disp.plot(ax=axes[1, 1], cmap=plt.cm.Blues, values_format=\"d\")\n",
    "axes[1, 1].set_title(\"Confusion Matrix\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5caa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haaland–Thomas Style PLSR with Cross-Validation RMSE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load dataset\n",
    "train_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/train_ks_2.csv\"\n",
    "test_path  = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/test_ks_2.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df  = pd.read_csv(test_path)\n",
    "\n",
    "target_column = \"Yield\"\n",
    "\n",
    "X_train = train_df.drop(columns=[target_column]).values\n",
    "y_train = train_df[target_column].values\n",
    "X_test = test_df.drop(columns=[target_column]).values\n",
    "y_test = test_df[target_column].values\n",
    "\n",
    "# 2. Standardize features\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled  = scaler_X.transform(X_test)\n",
    "\n",
    "# 3. Define PLSR model\n",
    "pls = PLSRegression(scale=False)\n",
    "\n",
    "# 4. Define range of latent variables (components)\n",
    "param_grid = {'n_components': list(range(1, min(12, X_train.shape[1]) + 1))}\n",
    "\n",
    "# 5. Define cross-validation strategy\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# 6. GridSearchCV using negative MSE (Haaland–Thomas criterion = lowest RMSECV)\n",
    "grid = GridSearchCV(\n",
    "    estimator=pls,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# 7. Fit grid search\n",
    "print(\"\\nRunning Haaland–Thomas PLSR...\")\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 8. Extract best model and parameters\n",
    "best_ncomp = grid.best_params_['n_components']\n",
    "best_pls = grid.best_estimator_\n",
    "print(f\"\\nOptimal number of components (Haaland–Thomas): {best_ncomp}\")\n",
    "print(f\"Best cross-validation RMSE: {np.sqrt(-grid.best_score_):.4f}\")\n",
    "\n",
    "# 9. Model evaluation (Train/Test)\n",
    "y_pred_train = best_pls.predict(X_train_scaled).ravel()\n",
    "y_pred_test  = best_pls.predict(X_test_scaled).ravel()\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rpd = np.std(y_true, ddof=1) / rmse\n",
    "    return r2, rmse, rpd\n",
    "\n",
    "train_r2, train_rmse, train_rpd = evaluate(y_train, y_pred_train)\n",
    "test_r2, test_rmse, test_rpd = evaluate(y_test, y_pred_test)\n",
    "\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(f\"Train R²   : {train_r2:.4f}\")\n",
    "print(f\"Test  R²   : {test_r2:.4f}\")\n",
    "print(f\"Train RMSE : {train_rmse:.4f}\")\n",
    "print(f\"Test  RMSE : {test_rmse:.4f}\")\n",
    "print(f\"Train RPD  : {train_rpd:.4f}\")\n",
    "print(f\"Test  RPD  : {test_rpd:.4f}\")\n",
    "\n",
    "# 10. Plot RMSE vs number of components (Haaland–Thomas curve)\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "results['RMSECV'] = np.sqrt(-results['mean_test_score'])\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(results['param_n_components'], results['RMSECV'], marker='o', color='blue')\n",
    "plt.axvline(best_ncomp, color='red', linestyle='--', label=f'Best n_components = {best_ncomp}')\n",
    "plt.title(\"Haaland–Thomas RMSECV Curve for PLSR\")\n",
    "plt.xlabel(\"Number of PLS Components (Latent Variables)\")\n",
    "plt.ylabel(\"RMSECV (Cross-Validation)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52d9565",
   "metadata": {},
   "source": [
    "Section 4(RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e53eeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regression\n",
    "import pandas as pd                           # For handling CSV files and dataframes\n",
    "import numpy as np                            # For numerical calculations\n",
    "from sklearn.preprocessing import StandardScaler  # For scaling features to standard range\n",
    "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, ConfusionMatrixDisplay  # Metrics for evaluation\n",
    "from sklearn.ensemble import RandomForestRegressor   # Random Forest Regressor model\n",
    "import matplotlib.pyplot as plt               # For plotting graphs\n",
    "import pickle                                 # For saving/loading model and scaler\n",
    "\n",
    "# Load CSV files\n",
    "train_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/train_ks_2.csv\"  # Training dataset path\n",
    "test_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/test_ks_2.csv\"    # Testing dataset path\n",
    "\n",
    "train_df = pd.read_csv(train_path)            # Read training CSV into DataFrame\n",
    "test_df = pd.read_csv(test_path)              # Read testing CSV into DataFrame\n",
    "\n",
    "# Features and target\n",
    "target_column = \"Yield\"                       # The target column to predict is 'Yield'\n",
    "X_train = train_df.drop(columns=[target_column]).values   # Training features (all columns except 'Yield')\n",
    "y_train = train_df[target_column].values      # Training target (Yield values)\n",
    "X_test = test_df.drop(columns=[target_column]).values     # Testing features\n",
    "y_test = test_df[target_column].values        # Testing target\n",
    "\n",
    "# Standardize features\n",
    "scaler_X = StandardScaler()                   # Create a scaler to standardize features (mean=0, std=1)\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)    # Fit scaler on training data and transform it\n",
    "X_test_scaled = scaler_X.transform(X_test)          # Use the same scaler to transform test data\n",
    "\n",
    "# Save the scaler\n",
    "with open(\"scaler_X.pkl\", \"wb\") as f:         # Open file in write-binary mode\n",
    "    pickle.dump(scaler_X, f)                  # Save the fitted scaler for future use\n",
    "\n",
    "# Build and train Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=400,       # Number of trees in the forest\n",
    "    max_depth=20, \n",
    "    min_samples_split=2,    # Minimum samples required to split a node\n",
    "    min_samples_leaf=2,           # Maximum depth of each tree (limits overfitting)\n",
    "    random_state=42,        # Set seed for reproducibility\n",
    "    n_jobs=-1               # Use all available CPU cores for faster training\n",
    ")\n",
    "rf_model.fit(X_train_scaled, y_train)         # Train the Random Forest model using scaled training data\n",
    "\n",
    "# Save model\n",
    "with open(\"RF_model.pkl\", \"wb\") as f:         # Open file to save trained model\n",
    "    pickle.dump(rf_model, f)                  # Save the trained Random Forest model\n",
    "print(\"Random Forest model saved as RF_model.pkl\")   # Print confirmation\n",
    "\n",
    "# Load saved model and scaler\n",
    "rf_loaded = pickle.load(open(\"RF_model.pkl\", \"rb\"))        # Load the saved Random Forest model\n",
    "scaler_loaded = pickle.load(open(\"scaler_X.pkl\", \"rb\"))    # Load the saved scaler\n",
    "\n",
    "# Scale test/train data using loaded scaler\n",
    "X_train_scaled_loaded = scaler_loaded.transform(X_train)   # Scale train data again using loaded scaler\n",
    "X_test_scaled_loaded = scaler_loaded.transform(X_test)     # Scale test data again using loaded scaler\n",
    "\n",
    "# Predict\n",
    "y_pred_train = rf_loaded.predict(X_train_scaled_loaded)    # Predict yields on training data\n",
    "y_pred_test = rf_loaded.predict(X_test_scaled_loaded)      # Predict yields on testing data\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(y_true, y_pred):                               # Define a function for model evaluation\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))      # Calculate Root Mean Square Error\n",
    "    r2 = r2_score(y_true, y_pred)                           # Calculate R² score (goodness of fit)\n",
    "    rpd = np.std(y_true, ddof=1) / rmse                     # Calculate RPD (useful in agriculture)\n",
    "    return r2, rmse, rpd                                    # Return the metrics\n",
    "\n",
    "# Evaluate predictions\n",
    "train_r2, train_rmse, train_rpd = evaluate(y_train, y_pred_train)  # Evaluate training data performance\n",
    "test_r2, test_rmse, test_rpd = evaluate(y_test, y_pred_test)       # Evaluate testing data performance\n",
    "\n",
    "# Print results\n",
    "print(\"\\nRandom Forest Model Evaluation:\")                 # Print header\n",
    "print(f\"Train R²   : {train_r2:.4f}\")                       # Show R² for training data\n",
    "print(f\"Test R²    : {test_r2:.4f}\")                        # Show R² for testing data\n",
    "print(f\"Train RMSE : {train_rmse:.4f}\")                     # Show RMSE for training data\n",
    "print(f\"Test RMSE  : {test_rmse:.4f}\")                       # Show RMSE for testing data\n",
    "print(f\"Train RPD  : {train_rpd:.4f}\")                       # Show RPD for training data\n",
    "print(f\"Test RPD   : {test_rpd:.4f}\")                         # Show RPD for testing data\n",
    "\n",
    "# Plots (similar to CNN code)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14,12))              # Create 2x2 grid of subplots\n",
    "\n",
    "# Train scatter plot\n",
    "axes[0,0].scatter(y_train, y_pred_train, color='blue', alpha=0.6, label='Train Predicted')    # Scatter plot for train data\n",
    "z_train = np.polyfit(y_train, y_pred_train, 1)               # Fit a best-fit line for train data\n",
    "p_train = np.poly1d(z_train)                                 # Create polynomial function\n",
    "axes[0,0].plot(y_train, p_train(y_train), \"blue\", linestyle='--', label=\"Train Best Fit\")    # Plot best-fit line\n",
    "axes[0,0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \"green\", linestyle='-', label=\"Ideal Fit\")  # Ideal diagonal line\n",
    "axes[0,0].set_title(\"Training Set: Predicted vs Actual\")     # Set title\n",
    "axes[0,0].set_xlabel(\"Actual Values\")                        # X-axis label\n",
    "axes[0,0].set_ylabel(\"Predicted Values\")                     # Y-axis label\n",
    "axes[0,0].legend()                                           # Show legend\n",
    "axes[0,0].grid(True)                                         # Add grid lines\n",
    "\n",
    "# Test scatter plot\n",
    "axes[0,1].scatter(y_test, y_pred_test, color='red', alpha=0.6, label='Test Predicted')       # Scatter plot for test data\n",
    "z_test = np.polyfit(y_test, y_pred_test, 1)                  # Fit a best-fit line for test data\n",
    "p_test = np.poly1d(z_test)                                   # Create polynomial function\n",
    "axes[0,1].plot(y_test, p_test(y_test), \"orange\", linestyle='--', label=\"Test Best Fit\")     # Plot best-fit line\n",
    "axes[0,1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"green\", linestyle='-', label=\"Ideal Fit\")      # Ideal diagonal line\n",
    "axes[0,1].set_title(\"Test Set: Predicted vs Actual\")         # Set title\n",
    "axes[0,1].set_xlabel(\"Actual Values\")                        # X-axis label\n",
    "axes[0,1].set_ylabel(\"Predicted Values\")                     # Y-axis label\n",
    "axes[0,1].legend()                                           # Show legend\n",
    "axes[0,1].grid(True)                                         # Add grid lines\n",
    "\n",
    "# Feature importance plot\n",
    "axes[1,0].bar(range(X_train.shape[1]), rf_loaded.feature_importances_, color='purple')      # Bar chart for feature importance\n",
    "axes[1,0].set_title(\"Random Forest Feature Importances\")     # Title of the chart\n",
    "axes[1,0].set_xlabel(\"Feature Index\")                         # X-axis label\n",
    "axes[1,0].set_ylabel(\"Importance\")                            # Y-axis label\n",
    "axes[1,0].grid(True)                                          # Add grid lines\n",
    "\n",
    "# Confusion matrix (convert regression output to classes)\n",
    "y_test_class = (y_test >= np.median(y_test)).astype(int)      # Classify yields as High (1) or Low (0) based on median\n",
    "y_pred_test_class = (y_pred_test >= np.median(y_test)).astype(int)  # Same for predictions\n",
    "cm = confusion_matrix(y_test_class, y_pred_test_class)        # Compute confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Low Yield\", \"High Yield\"]) # Create display object\n",
    "disp.plot(ax=axes[1,1], cmap=plt.cm.Blues, values_format=\"d\")  # Plot confusion matrix with blue color map\n",
    "axes[1,1].set_title(\"Confusion Matrix\")                       # Set title\n",
    "\n",
    "plt.tight_layout()                                             # Adjust layout to avoid overlap\n",
    "plt.show()                                                     # Display all plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca66618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regression + RandomizedSearchCV\n",
    "import pandas as pd  # For handling CSV files and DataFrames\n",
    "import numpy as np  # For numerical operations like arrays and math\n",
    "from sklearn.preprocessing import StandardScaler  # To standardize features (mean=0, std=1)\n",
    "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, ConfusionMatrixDisplay  # For evaluation\n",
    "from sklearn.ensemble import RandomForestRegressor  # Random Forest model\n",
    "from sklearn.model_selection import RandomizedSearchCV  # Hyperparameter tuning using randomized search\n",
    "from scipy.stats import randint  # To define random integer distributions for hyperparameters\n",
    "import matplotlib.pyplot as plt  # For plotting graphs\n",
    "import pickle  # To save/load trained models or scalers\n",
    "\n",
    "# 1. Load CSV files\n",
    "train_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/train_ks_2.csv\"  # Path to training CSV\n",
    "test_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/test_ks_2.csv\"    # Path to testing CSV\n",
    "\n",
    "train_df = pd.read_csv(train_path)  # Load training data into DataFrame\n",
    "test_df = pd.read_csv(test_path)    # Load testing data into DataFrame\n",
    "\n",
    "# 2. Features and target\n",
    "target_column = \"Yield\"  # Column we want to predict\n",
    "X_train = train_df.drop(columns=[target_column]).values  # Training features (all columns except target)\n",
    "y_train = train_df[target_column].values  # Training target values\n",
    "X_test = test_df.drop(columns=[target_column]).values  # Testing features\n",
    "y_test = test_df[target_column].values  # Testing target values\n",
    "\n",
    "# 3. Standardize features\n",
    "scaler_X = StandardScaler()  # Initialize scaler\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)  # Fit on training data and transform\n",
    "X_test_scaled = scaler_X.transform(X_test)  # Transform test data using same scaler\n",
    "\n",
    "with open(\"scaler_X.pkl\", \"wb\") as f:  # Save scaler for future use\n",
    "    pickle.dump(scaler_X, f)\n",
    "\n",
    "# 4. Hyperparameter Tuning with RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': [400, 500, 600],      # Number of trees; more → stable but slower\n",
    "    'max_depth': [3, 4, 5],               # Max depth of trees; lower → less overfitting\n",
    "    'min_samples_split': [20, 25, 30, 35, 40], # Min samples to split a node; higher → more regularization\n",
    "    'min_samples_leaf': [8, 10, 12, 15], # Min samples at leaf; prevents tiny leaves\n",
    "    'max_features': [0.3, 0.4, 0.5],     # Fraction of features per split; lower → more randomness\n",
    "    'bootstrap': [True],                  # Use bootstrap sampling\n",
    "    'max_samples': [0.7]                  # Fraction of samples per tree\n",
    "}\n",
    "\n",
    "rf_model = RandomForestRegressor(random_state=42)  # Base RF model with reproducibility\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=15,          # Slightly more combinations to explore\n",
    "    scoring='r2',\n",
    "    cv=10,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\n Starting RandomizedSearchCV...\")\n",
    "random_search.fit(X_train_scaled, y_train)  # Run randomized search and fit on training data\n",
    "\n",
    "print(\"\\n Best Parameters found:\")\n",
    "print(random_search.best_params_)  # Print best hyperparameters\n",
    "print(\" Best R² Score from CV:\", random_search.best_score_)  # Print best cross-validated R² score\n",
    "\n",
    "best_rf = random_search.best_estimator_  # Extract the best model\n",
    "\n",
    "# 5. Save the Best Model\n",
    "with open(\"RF_model_best.pkl\", \"wb\") as f:  # Open file to save model\n",
    "    pickle.dump(best_rf, f)  # Save the trained model\n",
    "print(\"\\n Best Random Forest model saved as RF_model_best.pkl\")\n",
    "\n",
    "# 6. Predictions\n",
    "y_pred_train = best_rf.predict(X_train_scaled)  # Predict on training data\n",
    "y_pred_test = best_rf.predict(X_test_scaled)  # Predict on test data\n",
    "\n",
    "# 7. Evaluation Function\n",
    "def evaluate(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))  # Compute RMSE\n",
    "    r2 = r2_score(y_true, y_pred)  # Compute R²\n",
    "    rpd = np.std(y_true, ddof=1) / rmse  # Compute RPD\n",
    "    return r2, rmse, rpd  # Return metrics\n",
    "\n",
    "train_r2, train_rmse, train_rpd = evaluate(y_train, y_pred_train)  # Evaluate training predictions\n",
    "test_r2, test_rmse, test_rpd = evaluate(y_test, y_pred_test)  # Evaluate testing predictions\n",
    "\n",
    "print(\"\\n Random Forest Model Evaluation:\")\n",
    "print(f\"Train R²   : {train_r2:.4f}\")  # Training R²\n",
    "print(f\"Test R²    : {test_r2:.4f}\")  # Testing R²\n",
    "print(f\"Train RMSE : {train_rmse:.4f}\")  # Training RMSE\n",
    "print(f\"Test RMSE  : {test_rmse:.4f}\")  # Testing RMSE\n",
    "print(f\"Train RPD  : {train_rpd:.4f}\")  # Training RPD\n",
    "print(f\"Test RPD   : {test_rpd:.4f}\")  # Testing RPD\n",
    "\n",
    "# 8. Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))  # Create 2x2 subplot grid\n",
    "\n",
    "# Train Scatter Plot\n",
    "axes[0, 0].scatter(y_train, y_pred_train, color='blue', alpha=0.6, label='Train Predicted')  # Scatter plot\n",
    "z_train = np.polyfit(y_train, y_pred_train, 1)  # Fit line to training predictions\n",
    "p_train = np.poly1d(z_train)  # Polynomial for best-fit line\n",
    "axes[0, 0].plot(y_train, p_train(y_train), \"blue\", linestyle='--', label=\"Train Best Fit\")  # Plot best-fit\n",
    "axes[0, 0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \"green\", linestyle='-', label=\"Ideal Fit\")  # Ideal diagonal\n",
    "axes[0, 0].set_title(\"Training Set: Predicted vs Actual\")  # Title\n",
    "axes[0, 0].set_xlabel(\"Actual Values\")  # X-axis label\n",
    "axes[0, 0].set_ylabel(\"Predicted Values\")  # Y-axis label\n",
    "axes[0, 0].legend()  # Show legend\n",
    "axes[0, 0].grid(True)  # Show grid\n",
    "\n",
    "# Test Scatter Plot\n",
    "axes[0, 1].scatter(y_test, y_pred_test, color='red', alpha=0.6, label='Test Predicted')  # Scatter plot\n",
    "z_test = np.polyfit(y_test, y_pred_test, 1)  # Fit line to test predictions\n",
    "p_test = np.poly1d(z_test)  # Polynomial for best-fit\n",
    "axes[0, 1].plot(y_test, p_test(y_test), \"orange\", linestyle='--', label=\"Test Best Fit\")  # Best-fit line\n",
    "axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"green\", linestyle='-', label=\"Ideal Fit\")  # Ideal diagonal\n",
    "axes[0, 1].set_title(\"Test Set: Predicted vs Actual\")  # Title\n",
    "axes[0, 1].set_xlabel(\"Actual Values\")  # X-axis label\n",
    "axes[0, 1].set_ylabel(\"Predicted Values\")  # Y-axis label\n",
    "axes[0, 1].legend()  # Show legend\n",
    "axes[0, 1].grid(True)  # Show grid\n",
    "\n",
    "# Feature Importance Plot\n",
    "axes[1, 0].bar(range(X_train.shape[1]), best_rf.feature_importances_, color='purple')  # Bar plot for feature importance\n",
    "axes[1, 0].set_title(\"Random Forest Feature Importances\")  # Title\n",
    "axes[1, 0].set_xlabel(\"Feature Index\")  # X-axis label\n",
    "axes[1, 0].set_ylabel(\"Importance\")  # Y-axis label\n",
    "axes[1, 0].grid(True)  # Show grid\n",
    "\n",
    "# Confusion Matrix\n",
    "y_test_class = (y_test >= np.median(y_test)).astype(int)  # Convert true values to High/Low classes\n",
    "y_pred_test_class = (y_pred_test >= np.median(y_test)).astype(int)  # Convert predicted values to classes\n",
    "cm = confusion_matrix(y_test_class, y_pred_test_class)  # Compute confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Low Yield\", \"High Yield\"])  # Prepare display\n",
    "disp.plot(ax=axes[1, 1], cmap=plt.cm.Blues, values_format=\"d\")  # Plot confusion matrix\n",
    "axes[1, 1].set_title(\"Confusion Matrix\")  # Title\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.show()  # Display all plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3947de63",
   "metadata": {},
   "source": [
    "Section 4.1(Explainable Ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596a218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest + Hyperparameter Tuning + Explainable AI (SHAP)\n",
    "\n",
    "import pandas as pd                          # Data handling\n",
    "import numpy as np                           # Numerical operations\n",
    "from sklearn.preprocessing import StandardScaler    # Feature standardization\n",
    "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, ConfusionMatrixDisplay  # Evaluation metrics\n",
    "from sklearn.ensemble import RandomForestRegressor   # Random Forest model\n",
    "from sklearn.model_selection import RandomizedSearchCV  # Randomized hyperparameter search\n",
    "import matplotlib.pyplot as plt              # Visualization\n",
    "import pickle                                # Save & load model/scaler\n",
    "import shap                                  # SHAP for explainable AI\n",
    "import os                                    # File and folder operations\n",
    "\n",
    "# 1. Load Dataset\n",
    "train_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/train_ks_2.csv\"   # Training dataset path\n",
    "test_path  = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/test_ks_2.csv\"    # Testing dataset path\n",
    "\n",
    "train_df = pd.read_csv(train_path)           # Read training CSV\n",
    "test_df  = pd.read_csv(test_path)            # Read testing CSV\n",
    "\n",
    "# 2. Feature-Target Split\n",
    "target_column = \"Yield\"                      # Target variable name\n",
    "X_train = train_df.drop(columns=[target_column]).values   # Features from train set\n",
    "y_train = train_df[target_column].values                   # Target from train set\n",
    "X_test  = test_df.drop(columns=[target_column]).values     # Features from test set\n",
    "y_test  = test_df[target_column].values                    # Target from test set\n",
    "\n",
    "# 3. Standardization (scaling features)\n",
    "scaler_X = StandardScaler()                  # Initialize scaler\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)   # Fit & scale training data\n",
    "X_test_scaled  = scaler_X.transform(X_test)        # Scale test data using same parameters\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)         # Create \"models\" folder if not already exists\n",
    "with open(\"models/scaler_X.pkl\", \"wb\") as f: # Save scaler object for future predictions\n",
    "    pickle.dump(scaler_X, f)\n",
    "\n",
    "# 4. Random Forest + Hyperparameter Tuning\n",
    "param_dist = {                               # Define parameter search space\n",
    "    'n_estimators': [400, 500, 600],         # Number of trees\n",
    "    'max_depth': [3, 4, 5],                  # Maximum tree depth\n",
    "    'min_samples_split': [20, 25, 30, 35, 40],  # Minimum samples to split a node\n",
    "    'min_samples_leaf': [8, 10, 12, 15],     # Minimum samples at a leaf\n",
    "    'max_features': [0.3, 0.4, 0.5],         # Fraction of features per split\n",
    "    'bootstrap': [True],                     # Use bootstrap samples\n",
    "    'max_samples': [0.7]                     # Fraction of samples for each tree\n",
    "}\n",
    "\n",
    "rf_model = RandomForestRegressor(random_state=42)  # Base Random Forest model\n",
    "\n",
    "random_search = RandomizedSearchCV(                # Random search for tuning\n",
    "    estimator=rf_model,                            # Base model\n",
    "    param_distributions=param_dist,                # Search space\n",
    "    n_iter=15,                                     # Number of random combos\n",
    "    scoring='r2',                                  # Scoring metric (R²)\n",
    "    cv=10,                                         # 10-fold cross-validation\n",
    "    n_jobs=-1,                                     # Use all CPU cores\n",
    "    verbose=2,                                     # Show progress\n",
    "    random_state=42                                # For reproducibility\n",
    ")\n",
    "\n",
    "print(\"\\nStarting RandomizedSearchCV...\")          \n",
    "random_search.fit(X_train_scaled, y_train)         # Fit search on training data\n",
    "\n",
    "print(\"\\nBest Parameters found:\")                  \n",
    "print(random_search.best_params_)                  # Print best hyperparameters\n",
    "print(f\"Best R² Score from CV: {random_search.best_score_:.4f}\")  # Print best cross-validation score\n",
    "\n",
    "best_rf = random_search.best_estimator_            # Extract best model\n",
    "\n",
    "with open(\"models/RF_model_best.pkl\", \"wb\") as f:  # Save best model to disk\n",
    "    pickle.dump(best_rf, f)\n",
    "print(\"\\nBest Random Forest model saved as models/RF_model_best.pkl\")\n",
    "\n",
    "# 5. Model Evaluation\n",
    "y_pred_train = best_rf.predict(X_train_scaled)     # Predict on training data\n",
    "y_pred_test  = best_rf.predict(X_test_scaled)      # Predict on test data\n",
    "\n",
    "def evaluate(y_true, y_pred):                      # Custom evaluation function\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))  # Root Mean Square Error\n",
    "    r2 = r2_score(y_true, y_pred)                      # R² (goodness of fit)\n",
    "    rpd = np.std(y_true, ddof=1) / rmse                # Residual Predictive Deviation\n",
    "    return r2, rmse, rpd\n",
    "\n",
    "train_r2, train_rmse, train_rpd = evaluate(y_train, y_pred_train)  # Train metrics\n",
    "test_r2, test_rmse, test_rpd = evaluate(y_test, y_pred_test)       # Test metrics\n",
    "\n",
    "print(\"\\nRandom Forest Model Evaluation:\")\n",
    "print(f\"Train R²   : {train_r2:.4f}\")              \n",
    "print(f\"Test R²    : {test_r2:.4f}\")               \n",
    "print(f\"Train RMSE : {train_rmse:.4f}\")            \n",
    "print(f\"Test RMSE  : {test_rmse:.4f}\")             \n",
    "print(f\"Train RPD  : {train_rpd:.4f}\")             \n",
    "print(f\"Test RPD   : {test_rpd:.4f}\")              \n",
    "\n",
    "# 6. Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))   # Create 2x2 subplot layout\n",
    "\n",
    "# Train Scatter Plot\n",
    "axes[0, 0].scatter(y_train, y_pred_train, color='blue', alpha=0.6, label='Train Predicted')  # Actual vs Predicted\n",
    "z_train = np.polyfit(y_train, y_pred_train, 1)      # Fit linear regression line\n",
    "p_train = np.poly1d(z_train)\n",
    "axes[0, 0].plot(y_train, p_train(y_train), \"blue\", linestyle='--', label=\"Train Best Fit\")   # Best fit line\n",
    "axes[0, 0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()],\n",
    "                \"green\", linestyle='-', label=\"Ideal Fit\")                                   # Ideal 1:1 line\n",
    "axes[0, 0].set_title(\"Training Set: Predicted vs Actual\")\n",
    "axes[0, 0].set_xlabel(\"Actual Values\")\n",
    "axes[0, 0].set_ylabel(\"Predicted Values\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Test Scatter Plot\n",
    "axes[0, 1].scatter(y_test, y_pred_test, color='red', alpha=0.6, label='Test Predicted')\n",
    "z_test = np.polyfit(y_test, y_pred_test, 1)\n",
    "p_test = np.poly1d(z_test)\n",
    "axes[0, 1].plot(y_test, p_test(y_test), \"orange\", linestyle='--', label=\"Test Best Fit\")\n",
    "axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],\n",
    "                \"green\", linestyle='-', label=\"Ideal Fit\")\n",
    "axes[0, 1].set_title(\"Test Set: Predicted vs Actual\")\n",
    "axes[0, 1].set_xlabel(\"Actual Values\")\n",
    "axes[0, 1].set_ylabel(\"Predicted Values\")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Feature Importance\n",
    "axes[1, 0].bar(range(X_train.shape[1]), best_rf.feature_importances_, color='purple')\n",
    "axes[1, 0].set_title(\"Random Forest Feature Importances\")\n",
    "axes[1, 0].set_xlabel(\"Feature Index\")\n",
    "axes[1, 0].set_ylabel(\"Importance\")\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Confusion Matrix\n",
    "y_test_class = (y_test >= np.median(y_test)).astype(int)          # Convert regression to binary (Low/High yield)\n",
    "y_pred_test_class = (y_pred_test >= np.median(y_test)).astype(int)\n",
    "cm = confusion_matrix(y_test_class, y_pred_test_class)            # Compute confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=[\"Low Yield\", \"High Yield\"])\n",
    "disp.plot(ax=axes[1, 1], cmap=plt.cm.Blues, values_format=\"d\")    # Plot matrix\n",
    "axes[1, 1].set_title(\"Confusion Matrix\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Explainable AI (SHAP)\n",
    "print(\"\\nCalculating SHAP values for Explainable AI visualization...\")\n",
    "\n",
    "explainer = shap.TreeExplainer(best_rf)         # Initialize SHAP explainer for RF\n",
    "shap_values = explainer.shap_values(X_test_scaled)  # Compute SHAP values for test data\n",
    "\n",
    "# Limit to Top 10 Most Important Features\n",
    "feature_names = test_df.drop(columns=[target_column]).columns          # Feature names\n",
    "mean_abs_shap = np.abs(shap_values).mean(axis=0)                       # Mean |SHAP| value per feature\n",
    "top_indices = np.argsort(mean_abs_shap)[-10:][::-1]                    # Top 10 indices (descending)\n",
    "\n",
    "X_test_top = X_test_scaled[:, top_indices]                             # Select top 10 features\n",
    "shap_values_top = shap_values[:, top_indices]                          # Corresponding SHAP values\n",
    "top_feature_names = feature_names[top_indices]                         # Top 10 feature names\n",
    "\n",
    "# SHAP Summary Plot (Top 10)\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(\n",
    "    shap_values_top,              # SHAP values (top 10)\n",
    "    X_test_top,                   # Data (top 10)\n",
    "    feature_names=top_feature_names,  # Feature names\n",
    "    show=False\n",
    ")\n",
    "plt.title(\"SHAP Summary Plot\", fontsize=13)\n",
    "plt.xlabel(\"SHAP value (impact on model output)\", fontsize=11)\n",
    "plt.ylabel(\"Wavelengths (nm)\", fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# SHAP Feature Importance Plot (Top 10)\n",
    "plt.figure(figsize=(8, 5))\n",
    "shap.summary_plot(\n",
    "    shap_values_top,\n",
    "    X_test_top,\n",
    "    feature_names=top_feature_names,\n",
    "    plot_type=\"bar\",              # Bar chart for importance ranking\n",
    "    show=False\n",
    ")\n",
    "plt.title(\"SHAP Feature Importance\", fontsize=13)\n",
    "plt.xlabel(\"mean SHAP values\", fontsize=11)\n",
    "plt.ylabel(\"Wavelengths (nm)\", fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSHAP visualizations (Top 10 wavelengths) displayed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fb6382",
   "metadata": {},
   "source": [
    "Section 4.2(Genetic algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581b2164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest + Genetic Algorithm Feature Selection + SHAP \n",
    "import pandas as pd                              # Data manipulation and analysis\n",
    "import numpy as np                               # Numerical computations\n",
    "from sklearn.preprocessing import StandardScaler # Feature standardization\n",
    "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, ConfusionMatrixDisplay  # Model evaluation metrics\n",
    "from sklearn.ensemble import RandomForestRegressor  # Random Forest algorithm\n",
    "from sklearn.model_selection import RandomizedSearchCV  # Hyperparameter tuning\n",
    "import matplotlib.pyplot as plt                 # Data visualization\n",
    "import shap                                     # Explainable AI for model interpretation\n",
    "import pickle                                   # Model saving/loading\n",
    "import os                                       # Operating system interface\n",
    "import random                                   # Random number generation\n",
    "\n",
    "# 1. Load Dataset\n",
    "train_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/train_ks_2.csv\"  # Training data file path\n",
    "test_path  = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/test_ks_2.csv\"   # Testing data file path\n",
    "\n",
    "train_df = pd.read_csv(train_path)              # Load training dataset into DataFrame\n",
    "test_df  = pd.read_csv(test_path)               # Load testing dataset into DataFrame\n",
    "\n",
    "# 2. Feature-Target Split\n",
    "target_column = \"Yield\"                         # Name of the target variable column\n",
    "X_train_full = train_df.drop(columns=[target_column]).values  # Training features (all columns except target)\n",
    "y_train = train_df[target_column].values                      # Training target values\n",
    "X_test_full  = test_df.drop(columns=[target_column]).values   # Testing features (all columns except target)\n",
    "y_test  = test_df[target_column].values                       # Testing target values\n",
    "feature_names = train_df.drop(columns=[target_column]).columns  # Get names of all features\n",
    "\n",
    "# 3. Standardization\n",
    "scaler_X = StandardScaler()                     # Initialize standard scaler for feature normalization\n",
    "X_train_scaled = scaler_X.fit_transform(X_train_full)  # Fit scaler on training data and transform features\n",
    "X_test_scaled  = scaler_X.transform(X_test_full)       # Transform test data using fitted scaler\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)            # Create directory for saving models if it doesn't exist\n",
    "with open(\"models/scaler_X.pkl\", \"wb\") as f:    # Open file to save scaler object\n",
    "    pickle.dump(scaler_X, f)                    # Save scaler for future use\n",
    "\n",
    "# 4. Genetic Algorithm for Feature Selection \n",
    "POP_SIZE = 10                                   # Number of individuals in population\n",
    "N_GEN = 5                                      # Number of generations to evolve\n",
    "MUTATION_RATE = 0.2                            # Probability of mutation for each gene\n",
    "N_FEATURES = X_train_scaled.shape[1]           # Total number of features in dataset\n",
    "\n",
    "def fitness_function(individual):               # Function to evaluate quality of feature subset\n",
    "    selected_features = [i for i, bit in enumerate(individual) if bit == 1]  # Get indices of selected features\n",
    "    if len(selected_features) == 0:             # Check if no features are selected\n",
    "        return 0  # No feature selected = worst fitness  # Return zero fitness if no features\n",
    "    X_sel = X_train_scaled[:, selected_features]  # Extract selected features from training data\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)  # Initialize RF model\n",
    "    model.fit(X_sel, y_train)                   # Train model on selected features\n",
    "    y_pred = model.predict(X_sel)               # Make predictions on training data\n",
    "    return r2_score(y_train, y_pred)            # Return R² score as fitness measure\n",
    "\n",
    "def generate_individual():                      # Function to create random individual\n",
    "    return [random.choice([0, 1]) for _ in range(N_FEATURES)]  # Binary vector representing feature selection\n",
    "\n",
    "def crossover(parent1, parent2):                # Function to perform crossover between two parents\n",
    "    point = random.randint(1, N_FEATURES - 1)   # Randomly select crossover point\n",
    "    child1 = parent1[:point] + parent2[point:]  # Create child1 by combining parent segments\n",
    "    child2 = parent2[:point] + parent1[point:]  # Create child2 by combining parent segments\n",
    "    return child1, child2                       # Return both children\n",
    "\n",
    "def mutate(individual):                         # Function to perform mutation on individual\n",
    "    return [1 - bit if random.random() < MUTATION_RATE else bit for bit in individual]  # Flip bits based on mutation rate\n",
    "\n",
    "# Initialize population\n",
    "population = [generate_individual() for _ in range(POP_SIZE)]  # Create initial random population\n",
    "\n",
    "# Genetic Algorithm Main Loop\n",
    "for gen in range(N_GEN):                        # Iterate through generations\n",
    "    fitness_scores = [fitness_function(ind) for ind in population]  # Calculate fitness for each individual\n",
    "    sorted_pop = [ind for _, ind in sorted(zip(fitness_scores, population), key=lambda x: x[0], reverse=True)]  # Sort population by fitness\n",
    "    population = sorted_pop[:POP_SIZE//2]  # Keep top half (selection)  # Select best half of population\n",
    "\n",
    "    # Create children\n",
    "    children = []                               # Initialize list for new children\n",
    "    while len(children) < POP_SIZE - len(population):  # Continue until population is filled\n",
    "        parent1, parent2 = random.sample(population, 2)  # Randomly select two parents\n",
    "        child1, child2 = crossover(parent1, parent2)  # Perform crossover to create children\n",
    "        children.append(mutate(child1))         # Mutate child1 and add to children list\n",
    "        if len(children) < POP_SIZE - len(population):  # Check if more children needed\n",
    "            children.append(mutate(child2))     # Mutate child2 and add to children list\n",
    "\n",
    "    population += children                      # Add children to population\n",
    "    best_score = max(fitness_scores)           # Get best fitness score of generation\n",
    "    print(f\"Generation {gen+1}/{N_GEN} | Best R² = {best_score:.4f}\")  # Print generation progress\n",
    "\n",
    "# Final best individual\n",
    "final_fitness = [fitness_function(ind) for ind in population]  # Calculate final fitness scores\n",
    "best_individual = population[np.argmax(final_fitness)]  # Select individual with highest fitness\n",
    "selected_indices = [i for i, bit in enumerate(best_individual) if bit == 1]  # Get indices of selected features\n",
    "selected_feature_names = feature_names[selected_indices]  # Get names of selected features\n",
    "\n",
    "print(\"\\n Best Feature Subset Selected:\")       # Print selection results header\n",
    "print(selected_feature_names.tolist())         # Print list of selected feature names\n",
    "print(f\"Total Selected Features: {len(selected_indices)}\")  # Print count of selected features\n",
    "\n",
    "# Use only selected features for model training/testing\n",
    "X_train_sel = X_train_scaled[:, selected_indices]  # Create training set with selected features\n",
    "X_test_sel  = X_test_scaled[:, selected_indices]   # Create test set with selected features\n",
    "\n",
    "# 5. Random Forest + Hyperparameter Tuning \n",
    "param_dist = {                                  # Define hyperparameter search space\n",
    "    'n_estimators': [400, 500, 600],           # Number of trees in forest\n",
    "    'max_depth': [3, 4, 5],                    # Maximum depth of trees\n",
    "    'min_samples_split': [20, 25, 30, 35, 40], # Minimum samples required to split node\n",
    "    'min_samples_leaf': [8, 10, 12, 15],       # Minimum samples required at leaf node\n",
    "    'max_features': [0.3, 0.4, 0.5],           # Number of features to consider for split\n",
    "    'bootstrap': [True],                        # Whether to use bootstrap samples\n",
    "    'max_samples': [0.7]                       # Fraction of samples for each tree\n",
    "}\n",
    "\n",
    "rf_model = RandomForestRegressor(random_state=42)  # Initialize base Random Forest model\n",
    "random_search = RandomizedSearchCV(            # Initialize randomized parameter search\n",
    "    estimator=rf_model,                        # Model to tune\n",
    "    param_distributions=param_dist,            # Parameter distribution to sample from\n",
    "    n_iter=15,                                 # Number of parameter settings to sample\n",
    "    scoring='r2',                              # Metric to evaluate performance\n",
    "    cv=10,                                     # Number of cross-validation folds\n",
    "    n_jobs=-1,                                 # Use all available processors\n",
    "    verbose=2,                                 # Print progress messages\n",
    "    random_state=42                            # Seed for reproducibility\n",
    ")\n",
    "\n",
    "print(\"\\n Starting RandomizedSearchCV with GA-selected features...\")  # Print tuning start message\n",
    "random_search.fit(X_train_sel, y_train)        # Perform hyperparameter tuning\n",
    "\n",
    "print(\"\\nBest Parameters found:\")               # Print best parameters header\n",
    "print(random_search.best_params_)              # Print best found hyperparameters\n",
    "print(f\"Best CV R² Score: {random_search.best_score_:.4f}\")  # Print best cross-validation score\n",
    "\n",
    "best_rf = random_search.best_estimator_        # Get best performing model\n",
    "with open(\"models/RF_model_best.pkl\", \"wb\") as f:  # Open file to save best model\n",
    "    pickle.dump(best_rf, f)                    # Save best model to disk\n",
    "\n",
    "# 6. Model Evaluation \n",
    "def evaluate(y_true, y_pred):                  # Function to calculate evaluation metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))  # Calculate Root Mean Square Error\n",
    "    r2 = r2_score(y_true, y_pred)              # Calculate R-squared coefficient\n",
    "    rpd = np.std(y_true, ddof=1) / rmse        # Calculate Residual Predictive Deviation\n",
    "    return r2, rmse, rpd                       # Return all three metrics\n",
    "\n",
    "y_pred_train = best_rf.predict(X_train_sel)    # Make predictions on training data\n",
    "y_pred_test = best_rf.predict(X_test_sel)      # Make predictions on test data\n",
    "\n",
    "train_r2, train_rmse, train_rpd = evaluate(y_train, y_pred_train)  # Evaluate training performance\n",
    "test_r2, test_rmse, test_rpd = evaluate(y_test, y_pred_test)       # Evaluate test performance\n",
    "\n",
    "print(\"\\nRandom Forest Model Evaluation (with GA-selected features):\")  # Print evaluation header\n",
    "print(f\"Train R²   : {train_r2:.4f}\")          # Print training R² score\n",
    "print(f\"Test R²    : {test_r2:.4f}\")           # Print test R² score\n",
    "print(f\"Train RMSE : {train_rmse:.4f}\")        # Print training RMSE\n",
    "print(f\"Test RMSE  : {test_rmse:.4f}\")         # Print test RMSE\n",
    "print(f\"Train RPD  : {train_rpd:.4f}\")         # Print training RPD\n",
    "print(f\"Test RPD   : {test_rpd:.4f}\")          # Print test RPD\n",
    "\n",
    "# 7. Visualization \n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))  # Create 2x2 subplot figure\n",
    "\n",
    "# Train Scatter\n",
    "axes[0, 0].scatter(y_train, y_pred_train, color='blue', alpha=0.6, label='Train Predicted')  # Plot training predictions vs actual\n",
    "z_train = np.polyfit(y_train, y_pred_train, 1)  # Fit linear regression line to training data\n",
    "p_train = np.poly1d(z_train)                    # Create polynomial function from coefficients\n",
    "axes[0, 0].plot(y_train, p_train(y_train), \"blue\", linestyle='--', label=\"Train Best Fit\")  # Plot best fit line for training\n",
    "axes[0, 0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()],\n",
    "                \"green\", linestyle='-', label=\"Ideal Fit\")  # Plot ideal 1:1 line\n",
    "axes[0, 0].set_title(\"Training Set: Predicted vs Actual\")  # Set subplot title\n",
    "axes[0, 0].set_xlabel(\"Actual Values\")          # Set x-axis label\n",
    "axes[0, 0].set_ylabel(\"Predicted Values\")       # Set y-axis label\n",
    "axes[0, 0].legend()                             # Show legend\n",
    "axes[0, 0].grid(True)                           # Enable grid\n",
    "\n",
    "# Test Scatter\n",
    "axes[0, 1].scatter(y_test, y_pred_test, color='red', alpha=0.6, label='Test Predicted')  # Plot test predictions vs actual\n",
    "z_test = np.polyfit(y_test, y_pred_test, 1)     # Fit linear regression line to test data\n",
    "p_test = np.poly1d(z_test)                      # Create polynomial function from coefficients\n",
    "axes[0, 1].plot(y_test, p_test(y_test), \"orange\", linestyle='--', label=\"Test Best Fit\")  # Plot best fit line for test\n",
    "axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],\n",
    "                \"green\", linestyle='-', label=\"Ideal Fit\")  # Plot ideal 1:1 line\n",
    "axes[0, 1].set_title(\"Test Set: Predicted vs Actual\")  # Set subplot title\n",
    "axes[0, 1].set_xlabel(\"Actual Values\")          # Set x-axis label\n",
    "axes[0, 1].set_ylabel(\"Predicted Values\")       # Set y-axis label\n",
    "axes[0, 1].legend()                             # Show legend\n",
    "axes[0, 1].grid(True)                           # Enable grid\n",
    "\n",
    "# Feature Importance\n",
    "# Feature Importance Visualization (Fixed X-axis Range) \n",
    "full_importances = np.zeros(N_FEATURES)         # Initialize array for all feature importances\n",
    "for idx, imp in zip(selected_indices, best_rf.feature_importances_):  # Loop through selected features\n",
    "    full_importances[idx] = imp                 # Assign importance to corresponding feature index\n",
    "\n",
    "axes[1, 0].bar(range(N_FEATURES), full_importances, color='purple')  # Create bar plot of feature importances\n",
    "axes[1, 0].set_title(\"Random Forest Feature Importances (GA Selected)\")  # Set subplot title\n",
    "axes[1, 0].set_xlabel(\"Feature Index (0–204)\")  # Set x-axis label with range\n",
    "axes[1, 0].set_ylabel(\"Importance\")             # Set y-axis label\n",
    "axes[1, 0].set_xlim(0, N_FEATURES - 1)   # Set x-axis limits to show all features  # Fix x-axis range\n",
    "axes[1, 0].grid(True)                           # Enable grid\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "y_test_class = (y_test >= np.median(y_test)).astype(int)  # Convert continuous target to binary classes\n",
    "y_pred_test_class = (y_pred_test >= np.median(y_test)).astype(int)  # Convert predictions to binary classes\n",
    "cm = confusion_matrix(y_test_class, y_pred_test_class)  # Calculate confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Low Yield\", \"High Yield\"])  # Create display object\n",
    "disp.plot(ax=axes[1, 1], cmap=plt.cm.Blues, values_format=\"d\")  # Plot confusion matrix\n",
    "axes[1, 1].set_title(\"Confusion Matrix\")        # Set subplot title\n",
    "\n",
    "plt.tight_layout()                              # Adjust subplot spacing\n",
    "plt.show()                                      # Display all plots\n",
    "\n",
    "# 8. Explainable AI (SHAP) \n",
    "print(\"\\n Calculating SHAP values for Explainable AI visualization...\")  # Print SHAP start message\n",
    "\n",
    "import matplotlib.pyplot as plt                 # Re-import for text formatting\n",
    "\n",
    "# SHAP Summary Plot\n",
    "plt.figure(figsize=(10, 6))                     # Create figure for SHAP summary plot\n",
    "shap.summary_plot(                              # Create SHAP summary plot\n",
    "    shap_values_top,                            # SHAP values for top features\n",
    "    X_test_top,                                 # Test data for top features\n",
    "    feature_names=top_feature_names,            # Names of top features\n",
    "    show=False                                  # Don't display immediately\n",
    ")\n",
    "\n",
    "# Bold all text inside the plot\n",
    "for text in plt.gcf().findobj(match=plt.Text):  # Find all text objects in figure\n",
    "    text.set_fontweight('bold')                 # Make text bold\n",
    "\n",
    "plt.title(\"SHAP Summary Plot\", fontsize=13, fontweight='bold')  # Set bold title\n",
    "plt.xlabel(\"SHAP value (impact on model output)\", fontsize=11, fontweight='bold')  # Set bold x-label\n",
    "plt.ylabel(\"Wavelengths (nm)\", fontsize=11, fontweight='bold')  # Set bold y-label\n",
    "plt.tight_layout()                              # Adjust plot layout\n",
    "plt.show()                                      # Display plot\n",
    "\n",
    "\n",
    "# SHAP Bar Plot\n",
    "plt.figure(figsize=(8, 5))                      # Create figure for SHAP bar plot\n",
    "shap.summary_plot(                              # Create SHAP bar plot\n",
    "    shap_values_top,                            # SHAP values for top features\n",
    "    X_test_top,                                 # Test data for top features\n",
    "    feature_names=top_feature_names,            # Names of top features\n",
    "    plot_type=\"bar\",                            # Create bar plot type\n",
    "    show=False                                  # Don't display immediately\n",
    ")\n",
    "\n",
    "# Bold all text inside the plot\n",
    "for text in plt.gcf().findobj(match=plt.Text):  # Find all text objects in figure\n",
    "    text.set_fontweight('bold')                 # Make text bold\n",
    "\n",
    "plt.title(\"SHAP Feature Importance\", fontsize=13, fontweight='bold')  # Set bold title\n",
    "plt.xlabel(\"mean SHAP values\", fontsize=11, fontweight='bold')  # Set bold x-label\n",
    "plt.ylabel(\"Wavelengths (nm)\", fontsize=11, fontweight='bold')  # Set bold y-label\n",
    "plt.tight_layout()                              # Adjust plot layout\n",
    "plt.show()                                      # Display plot\n",
    "\n",
    "print(\"\\n GA + RF + SHAP complete. Visualization generated successfully.\")  # Print completion message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb717cdb",
   "metadata": {},
   "source": [
    "Section 5(CNN-1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccceb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN-1D (Convolutional Neural Network – One Dimensional)\n",
    "import pandas as pd                                      # For handling CSV files and tabular data\n",
    "import numpy as np                                       # For numerical operations\n",
    "from sklearn.preprocessing import StandardScaler         # For feature standardization\n",
    "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, ConfusionMatrixDisplay  # Evaluation metrics\n",
    "import matplotlib.pyplot as plt                           # For plotting\n",
    "from tensorflow.keras.models import Sequential, load_model  # Build and load Keras models\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization  # CNN layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping     # Early stopping during training\n",
    "import pickle                                           # For saving and loading scaler objects\n",
    "\n",
    "# Load CSV files containing KS-split data\n",
    "train_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/train_ks_2.csv\"  # Path to training data\n",
    "test_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/test_ks_2.csv\"    # Path to test data\n",
    "\n",
    "train_df = pd.read_csv(train_path)                       # Load training data into a DataFrame\n",
    "test_df = pd.read_csv(test_path)                         # Load test data into a DataFrame\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "target_column = \"Yield\"                                  # Target column to predict\n",
    "X_train = train_df.drop(columns=[target_column]).values  # Extract features from training set\n",
    "y_train = train_df[target_column].values                 # Extract target values from training set\n",
    "X_test = test_df.drop(columns=[target_column]).values    # Extract features from test set\n",
    "y_test = test_df[target_column].values                   # Extract target values from test set\n",
    "\n",
    "# Standardize features to zero mean and unit variance\n",
    "scaler_X = StandardScaler()                               # Initialize StandardScaler\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)          # Fit on training data and transform\n",
    "X_test_scaled = scaler_X.transform(X_test)                # Transform test data with same scaler\n",
    "\n",
    "# Save the scaler object for later reuse\n",
    "with open(\"scaler_X.pkl\", \"wb\") as f:                     # Open file to save scaler\n",
    "    pickle.dump(scaler_X, f)                              # Save scaler object\n",
    "\n",
    "# Reshape data for 1D-CNN input: (samples, timesteps, features=1)\n",
    "X_train_scaled = X_train_scaled[..., np.newaxis]          # Add extra dimension for CNN input\n",
    "X_test_scaled = X_test_scaled[..., np.newaxis]            # Add extra dimension for CNN input\n",
    "\n",
    "# Build 1D-CNN model using Keras Sequential API\n",
    "model = Sequential([                                      # Initialize sequential model\n",
    "    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train_scaled.shape[1],1)),  # Conv layer with 64 filters\n",
    "    BatchNormalization(),                                 # Normalize activations\n",
    "    MaxPooling1D(pool_size=2),                            # Downsample by factor of 2\n",
    "    Dropout(0.2),                                         # Dropout for regularization\n",
    "\n",
    "    Conv1D(128, kernel_size=3, activation='relu'),       # Second Conv layer with 128 filters\n",
    "    BatchNormalization(),                                 # Normalize activations\n",
    "    MaxPooling1D(pool_size=2),                            # Downsample\n",
    "    Dropout(0.2),                                         # Dropout\n",
    "\n",
    "    Flatten(),                                            # Flatten output to feed Dense layers\n",
    "    Dense(128, activation='relu'),                        # Fully connected Dense layer\n",
    "    Dropout(0.2),                                         # Dropout\n",
    "    Dense(1, activation='linear')                         # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile model with optimizer and loss function\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])  # Use Adam optimizer and MSE loss\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)  # Stop if val_loss doesn't improve\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,           # Training features and target\n",
    "    epochs=100,                        # Maximum number of epochs\n",
    "    batch_size=16,                     # Batch size\n",
    "    validation_split=0.2,              # Use 20% of training data as validation\n",
    "    verbose=1,                         # Show progress\n",
    "    callbacks=[early_stop]             # Early stopping callback\n",
    ")\n",
    "\n",
    "# Save final trained model\n",
    "model.save(\"CNN1D_final_model.keras\") # Save model for reuse\n",
    "print(\"Model saved as CNN1D_final_model.keras\")  # Print confirmation\n",
    "\n",
    "# Reuse saved model\n",
    "model_loaded = load_model(\"CNN1D_final_model.keras\")  # Load saved model\n",
    "scaler_loaded = pickle.load(open(\"scaler_X.pkl\", \"rb\"))  # Load saved scaler\n",
    "\n",
    "# Apply scaler and reshape data for prediction\n",
    "X_train_scaled_loaded = scaler_loaded.transform(X_train)[..., np.newaxis]  # Scale and reshape training data\n",
    "X_test_scaled_loaded = scaler_loaded.transform(X_test)[..., np.newaxis]    # Scale and reshape test data\n",
    "\n",
    "# Predict target values\n",
    "y_pred_train = model_loaded.predict(X_train_scaled_loaded).flatten()  # Training predictions\n",
    "y_pred_test = model_loaded.predict(X_test_scaled_loaded).flatten()    # Test predictions\n",
    "\n",
    "# Function to evaluate predictions\n",
    "def evaluate(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))  # Compute RMSE\n",
    "    r2 = r2_score(y_true, y_pred)                       # Compute R² score\n",
    "    rpd = np.std(y_true, ddof=1) / rmse                # Compute RPD (Ratio of Performance to Deviation)\n",
    "    return r2, rmse, rpd                                # Return metrics\n",
    "\n",
    "# Evaluate train and test predictions\n",
    "train_r2, train_rmse, train_rpd = evaluate(y_train, y_pred_train)  # Train metrics\n",
    "test_r2, test_rmse, test_rpd = evaluate(y_test, y_pred_test)       # Test metrics\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"\\nLoaded 1D-CNN Model Evaluation:\")\n",
    "print(f\"Train R²   : {train_r2:.4f}\")        # Print R² for training\n",
    "print(f\"Test R²    : {test_r2:.4f}\")         # Print R² for testing\n",
    "print(f\"Train RMSE : {train_rmse:.4f}\")      # Print RMSE for training\n",
    "print(f\"Test RMSE  : {test_rmse:.4f}\")       # Print RMSE for testing\n",
    "print(f\"Train RPD  : {train_rpd:.4f}\")       # Print RPD for training\n",
    "print(f\"Test RPD   : {test_rpd:.4f}\")        # Print RPD for testing\n",
    "\n",
    "# Create 2x2 grid plots for visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14,12))  # Create figure with 2x2 subplots\n",
    "\n",
    "# 1. Train scatter plot: predicted vs actual\n",
    "axes[0,0].scatter(y_train, y_pred_train, color='blue', alpha=0.6, label='Train Predicted')  # Scatter plot\n",
    "z_train = np.polyfit(y_train, y_pred_train, 1)      # Fit line for predictions\n",
    "p_train = np.poly1d(z_train)\n",
    "axes[0,0].plot(y_train, p_train(y_train), \"blue\", linestyle='--', label=\"Train Best Fit\")  # Plot best fit line\n",
    "axes[0,0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \"green\", linestyle='-', label=\"Ideal Fit\")  # Ideal y=x line\n",
    "axes[0,0].set_title(\"Training Set: Predicted vs Actual\")  # Set title\n",
    "axes[0,0].set_xlabel(\"Actual Values\")                     # X-axis label\n",
    "axes[0,0].set_ylabel(\"Predicted Values\")                  # Y-axis label\n",
    "axes[0,0].legend()                                        # Show legend\n",
    "axes[0,0].grid(True)                                      # Show grid\n",
    "\n",
    "# 2. Test scatter plot: predicted vs actual\n",
    "axes[0,1].scatter(y_test, y_pred_test, color='red', alpha=0.6, label='Test Predicted')      # Scatter plot\n",
    "z_test = np.polyfit(y_test, y_pred_test, 1)       # Fit line\n",
    "p_test = np.poly1d(z_test)\n",
    "axes[0,1].plot(y_test, p_test(y_test), \"orange\", linestyle='--', label=\"Test Best Fit\")    # Best fit line\n",
    "axes[0,1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"green\", linestyle='-', label=\"Ideal Fit\")  # Ideal line\n",
    "axes[0,1].set_title(\"Test Set: Predicted vs Actual\")  # Title\n",
    "axes[0,1].set_xlabel(\"Actual Values\")                 # X label\n",
    "axes[0,1].set_ylabel(\"Predicted Values\")              # Y label\n",
    "axes[0,1].legend()                                    # Legend\n",
    "axes[0,1].grid(True)                                  # Grid\n",
    "\n",
    "# 3. Training & validation loss over epochs\n",
    "axes[1,0].plot(history.history['loss'], label='Training Loss', color='blue')  # Training loss plot\n",
    "axes[1,0].plot(history.history['val_loss'], label='Validation Loss', color='orange')  # Validation loss\n",
    "axes[1,0].set_title(\"1D-CNN Training & Validation Loss\")   # Title\n",
    "axes[1,0].set_xlabel(\"Epochs\")                              # X-axis label\n",
    "axes[1,0].set_ylabel(\"MSE Loss\")                             # Y-axis label\n",
    "axes[1,0].legend()                                           # Legend\n",
    "axes[1,0].grid(True)                                         # Grid\n",
    "\n",
    "# 4. Confusion matrix for regression-as-classification\n",
    "y_test_class = (y_test >= np.median(y_test)).astype(int)          # Convert regression target to binary\n",
    "y_pred_test_class = (y_pred_test >= np.median(y_test)).astype(int)  # Convert predictions\n",
    "cm = confusion_matrix(y_test_class, y_pred_test_class)            # Compute confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Low Yield\", \"High Yield\"])  # Display\n",
    "disp.plot(ax=axes[1,1], cmap=plt.cm.Blues, values_format=\"d\")     # Plot confusion matrix\n",
    "axes[1,1].set_title(\"Confusion Matrix\")                            # Set title\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to avoid overlap\n",
    "plt.show()          # Show all plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2546889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# CNN-1D (1D Convolutional Neural Network) with Bayesian Hyperparameter Tuning\n",
    "# ===============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pickle\n",
    "import keras_tuner as kt  # Keras Tuner\n",
    "\n",
    "# =========================\n",
    "# Load data\n",
    "# =========================\n",
    "train_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/train_ks_2.csv\"\n",
    "test_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/test_ks_2.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "target_column = \"Yield\"\n",
    "X_train = train_df.drop(columns=[target_column]).values\n",
    "y_train = train_df[target_column].values\n",
    "X_test = test_df.drop(columns=[target_column]).values\n",
    "y_test = test_df[target_column].values\n",
    "\n",
    "# =========================\n",
    "# Standardize features\n",
    "# =========================\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "with open(\"scaler_X.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler_X, f)\n",
    "\n",
    "# Reshape for CNN\n",
    "X_train_scaled = X_train_scaled[..., np.newaxis]\n",
    "X_test_scaled = X_test_scaled[..., np.newaxis]\n",
    "\n",
    "# =========================\n",
    "# Define model builder for Keras Tuner\n",
    "# =========================\n",
    "def build_cnn_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(\n",
    "        filters=hp.Int('filters_1', 32, 128, step=32),\n",
    "        kernel_size=hp.Choice('kernel_size_1', [3,5,7]),\n",
    "        activation='relu',\n",
    "        input_shape=(X_train_scaled.shape[1],1)\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Dropout(hp.Float('dropout_1', 0.2, 0.5, step=0.1)))\n",
    "\n",
    "    model.add(Conv1D(\n",
    "        filters=hp.Int('filters_2', 64, 256, step=64),\n",
    "        kernel_size=hp.Choice('kernel_size_2', [3,5]),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Dropout(hp.Float('dropout_2', 0.2, 0.5, step=0.1)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(\n",
    "        hp.Int('dense_units', 64, 256, step=64),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(Dropout(hp.Float('dropout_dense', 0.2, 0.5, step=0.1)))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Choice('lr', [1e-3, 1e-4, 1e-5])),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# Keras Tuner: Bayesian Optimization\n",
    "# =========================\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_cnn_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=15,\n",
    "    executions_per_trial=1,\n",
    "    directory='cnn_bayes_tuning',\n",
    "    project_name='leaf_yield_tuning'\n",
    ")\n",
    "\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Run search\n",
    "tuner.search(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[stop_early],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"\\n✅ Best Hyperparameters found:\")\n",
    "for key, value in best_hps.values.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Build and train best model\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[stop_early],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save final model\n",
    "model.save(\"CNN1D_final_model.keras\")\n",
    "print(\"Model saved as CNN1D_final_model.keras\")\n",
    "\n",
    "# =========================\n",
    "# Evaluation\n",
    "# =========================\n",
    "model_loaded = load_model(\"CNN1D_final_model.keras\")\n",
    "scaler_loaded = pickle.load(open(\"scaler_X.pkl\", \"rb\"))\n",
    "\n",
    "X_train_scaled_loaded = scaler_loaded.transform(X_train)[..., np.newaxis]\n",
    "X_test_scaled_loaded = scaler_loaded.transform(X_test)[..., np.newaxis]\n",
    "\n",
    "y_pred_train = model_loaded.predict(X_train_scaled_loaded).flatten()\n",
    "y_pred_test = model_loaded.predict(X_test_scaled_loaded).flatten()\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rpd = np.std(y_true, ddof=1) / rmse\n",
    "    return r2, rmse, rpd\n",
    "\n",
    "train_r2, train_rmse, train_rpd = evaluate(y_train, y_pred_train)\n",
    "test_r2, test_rmse, test_rpd = evaluate(y_test, y_pred_test)\n",
    "\n",
    "print(\"\\nLoaded 1D-CNN Model Evaluation:\")\n",
    "print(f\"Train R²   : {train_r2:.4f}\")\n",
    "print(f\"Test R²    : {test_r2:.4f}\")\n",
    "print(f\"Train RMSE : {train_rmse:.4f}\")\n",
    "print(f\"Test RMSE  : {test_rmse:.4f}\")\n",
    "print(f\"Train RPD  : {train_rpd:.4f}\")\n",
    "print(f\"Test RPD   : {test_rpd:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# 2x2 Plots\n",
    "# =========================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14,12))\n",
    "\n",
    "# 1. Train\n",
    "axes[0,0].scatter(y_train, y_pred_train, color='blue', alpha=0.6, label='Train Predicted')\n",
    "z_train = np.polyfit(y_train, y_pred_train, 1)\n",
    "p_train = np.poly1d(z_train)\n",
    "axes[0,0].plot(y_train, p_train(y_train), \"blue\", linestyle='--', label=\"Train Best Fit\")\n",
    "axes[0,0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \"green\", linestyle='-', label=\"Ideal Fit\")\n",
    "axes[0,0].set_title(\"Training Set: Predicted vs Actual\")\n",
    "axes[0,0].set_xlabel(\"Actual Values\")\n",
    "axes[0,0].set_ylabel(\"Predicted Values\")\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True)\n",
    "\n",
    "# 2. Test\n",
    "axes[0,1].scatter(y_test, y_pred_test, color='red', alpha=0.6, label='Test Predicted')\n",
    "z_test = np.polyfit(y_test, y_pred_test, 1)\n",
    "p_test = np.poly1d(z_test)\n",
    "axes[0,1].plot(y_test, p_test(y_test), \"orange\", linestyle='--', label=\"Test Best Fit\")\n",
    "axes[0,1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"green\", linestyle='-', label=\"Ideal Fit\")\n",
    "axes[0,1].set_title(\"Test Set: Predicted vs Actual\")\n",
    "axes[0,1].set_xlabel(\"Actual Values\")\n",
    "axes[0,1].set_ylabel(\"Predicted Values\")\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True)\n",
    "\n",
    "# 3. Training & Validation Loss\n",
    "axes[1,0].plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "axes[1,0].plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
    "axes[1,0].set_title(\"1D-CNN Training & Validation Loss\")\n",
    "axes[1,0].set_xlabel(\"Epochs\")\n",
    "axes[1,0].set_ylabel(\"MSE Loss\")\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True)\n",
    "\n",
    "# 4. Confusion matrix\n",
    "y_test_class = (y_test >= np.median(y_test)).astype(int)\n",
    "y_pred_test_class = (y_pred_test >= np.median(y_test)).astype(int)\n",
    "cm = confusion_matrix(y_test_class, y_pred_test_class)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Low Yield\", \"High Yield\"])\n",
    "disp.plot(ax=axes[1,1], cmap=plt.cm.Blues, values_format=\"d\")\n",
    "axes[1,1].set_title(\"Confusion Matrix\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0907177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# CNN-1D (1D Convolutional Neural Network) with Improved Bayesian Hyperparameter Tuning\n",
    "# ===============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import pickle\n",
    "import keras_tuner as kt  # Keras Tuner\n",
    "\n",
    "# =========================\n",
    "# Load data\n",
    "# =========================\n",
    "train_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/train_ks_2.csv\"\n",
    "test_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/test_ks_2.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "target_column = \"Yield\"\n",
    "X_train = train_df.drop(columns=[target_column]).values\n",
    "y_train = train_df[target_column].values\n",
    "X_test = test_df.drop(columns=[target_column]).values\n",
    "y_test = test_df[target_column].values\n",
    "\n",
    "# =========================\n",
    "# Standardize features\n",
    "# =========================\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "with open(\"scaler_X.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler_X, f)\n",
    "\n",
    "# Reshape for CNN\n",
    "X_train_scaled = X_train_scaled[..., np.newaxis]\n",
    "X_test_scaled = X_test_scaled[..., np.newaxis]\n",
    "\n",
    "# =========================\n",
    "# Define model builder for Keras Tuner (Improved CNN)\n",
    "# =========================\n",
    "def build_cnn_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # 1st Conv Block\n",
    "    model.add(Conv1D(\n",
    "        filters=hp.Int('filters_1', 64, 128, step=32),\n",
    "        kernel_size=hp.Choice('kernel_size_1', [3, 5]),\n",
    "        activation='relu',\n",
    "        input_shape=(X_train_scaled.shape[1], 1)\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Dropout(hp.Float('dropout_1', 0.1, 0.3, step=0.05)))\n",
    "\n",
    "    # 2nd Conv Block\n",
    "    model.add(Conv1D(\n",
    "        filters=hp.Int('filters_2', 128, 256, step=64),\n",
    "        kernel_size=hp.Choice('kernel_size_2', [3, 5]),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Dropout(hp.Float('dropout_2', 0.1, 0.3, step=0.05)))\n",
    "\n",
    "    # 3rd Conv Block (new)\n",
    "    model.add(Conv1D(\n",
    "        filters=hp.Int('filters_3', 128, 256, step=64),\n",
    "        kernel_size=hp.Choice('kernel_size_3', [3, 5]),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Dropout(hp.Float('dropout_3', 0.1, 0.3, step=0.05)))\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(\n",
    "        hp.Int('dense_units', 128, 512, step=128),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(Dropout(hp.Float('dropout_dense', 0.1, 0.3, step=0.05)))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Choice('lr', [1e-3, 5e-4, 1e-4])),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# =========================\n",
    "# Keras Tuner: Bayesian Optimization\n",
    "# =========================\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_cnn_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    directory='cnn_bayes_tuning_improved',\n",
    "    project_name='leaf_yield_tuning_v2'\n",
    ")\n",
    "\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Run search\n",
    "tuner.search(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=150,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[stop_early, reduce_lr],\n",
    "    verbose=1,\n",
    "    batch_size=32  # Default; tuner can override if added as hp parameter\n",
    ")\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"\\nBest Hyperparameters found:\")\n",
    "for key, value in best_hps.values.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Build and train best model\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=150,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[stop_early, reduce_lr],\n",
    "    verbose=1,\n",
    "    batch_size=best_hps.get('batch_size', 32)\n",
    ")\n",
    "\n",
    "# Save final model\n",
    "model.save(\"CNN1D_final_model_improved.keras\")\n",
    "print(\"Model saved as CNN1D_final_model_improved.keras\")\n",
    "\n",
    "# =========================\n",
    "# Evaluation\n",
    "# =========================\n",
    "model_loaded = load_model(\"CNN1D_final_model_improved.keras\")\n",
    "scaler_loaded = pickle.load(open(\"scaler_X.pkl\", \"rb\"))\n",
    "\n",
    "X_train_scaled_loaded = scaler_loaded.transform(X_train)[..., np.newaxis]\n",
    "X_test_scaled_loaded = scaler_loaded.transform(X_test)[..., np.newaxis]\n",
    "\n",
    "y_pred_train = model_loaded.predict(X_train_scaled_loaded).flatten()\n",
    "y_pred_test = model_loaded.predict(X_test_scaled_loaded).flatten()\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rpd = np.std(y_true, ddof=1) / rmse\n",
    "    return r2, rmse, rpd\n",
    "\n",
    "train_r2, train_rmse, train_rpd = evaluate(y_train, y_pred_train)\n",
    "test_r2, test_rmse, test_rpd = evaluate(y_test, y_pred_test)\n",
    "\n",
    "print(\"\\nLoaded 1D-CNN Model Evaluation:\")\n",
    "print(f\"Train R²   : {train_r2:.4f}\")\n",
    "print(f\"Test R²    : {test_r2:.4f}\")\n",
    "print(f\"Train RMSE : {train_rmse:.4f}\")\n",
    "print(f\"Test RMSE  : {test_rmse:.4f}\")\n",
    "print(f\"Train RPD  : {train_rpd:.4f}\")\n",
    "print(f\"Test RPD   : {test_rpd:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# Visualization\n",
    "# =========================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14,12))\n",
    "\n",
    "# 1. Train\n",
    "axes[0,0].scatter(y_train, y_pred_train, color='blue', alpha=0.6, label='Train Predicted')\n",
    "z_train = np.polyfit(y_train, y_pred_train, 1)\n",
    "p_train = np.poly1d(z_train)\n",
    "axes[0,0].plot(y_train, p_train(y_train), \"blue\", linestyle='--', label=\"Train Best Fit\")\n",
    "axes[0,0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \"green\", linestyle='-', label=\"Ideal Fit\")\n",
    "axes[0,0].set_title(\"Training Set: Predicted vs Actual\")\n",
    "axes[0,0].set_xlabel(\"Actual Values\")\n",
    "axes[0,0].set_ylabel(\"Predicted Values\")\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True)\n",
    "\n",
    "# 2. Test\n",
    "axes[0,1].scatter(y_test, y_pred_test, color='red', alpha=0.6, label='Test Predicted')\n",
    "z_test = np.polyfit(y_test, y_pred_test, 1)\n",
    "p_test = np.poly1d(z_test)\n",
    "axes[0,1].plot(y_test, p_test(y_test), \"orange\", linestyle='--', label=\"Test Best Fit\")\n",
    "axes[0,1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"green\", linestyle='-', label=\"Ideal Fit\")\n",
    "axes[0,1].set_title(\"Test Set: Predicted vs Actual\")\n",
    "axes[0,1].set_xlabel(\"Actual Values\")\n",
    "axes[0,1].set_ylabel(\"Predicted Values\")\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True)\n",
    "\n",
    "# 3. Training & Validation Loss\n",
    "axes[1,0].plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "axes[1,0].plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
    "axes[1,0].set_title(\"1D-CNN Training & Validation Loss\")\n",
    "axes[1,0].set_xlabel(\"Epochs\")\n",
    "axes[1,0].set_ylabel(\"MSE Loss\")\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True)\n",
    "\n",
    "# 4. Confusion matrix\n",
    "y_test_class = (y_test >= np.median(y_test)).astype(int)\n",
    "y_pred_test_class = (y_pred_test >= np.median(y_test)).astype(int)\n",
    "cm = confusion_matrix(y_test_class, y_pred_test_class)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Low Yield\", \"High Yield\"])\n",
    "disp.plot(ax=axes[1,1], cmap=plt.cm.Blues, values_format=\"d\")\n",
    "axes[1,1].set_title(\"Confusion Matrix\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba2d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# CNN-1D (PyTorch version) with Bayesian Hyperparameter Tuning (Optuna)\n",
    "# ===============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import pickle\n",
    "\n",
    "# =========================\n",
    "# Load data\n",
    "# =========================\n",
    "train_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/train_ks_2.csv\"\n",
    "test_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/test_ks_2.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "target_column = \"Yield\"\n",
    "X_train = train_df.drop(columns=[target_column]).values\n",
    "y_train = train_df[target_column].values\n",
    "X_test = test_df.drop(columns=[target_column]).values\n",
    "y_test = test_df[target_column].values\n",
    "\n",
    "# =========================\n",
    "# Standardize features\n",
    "# =========================\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "with open(\"scaler_X.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler_X, f)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).unsqueeze(1)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).unsqueeze(1)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# =========================\n",
    "# Define CNN Model\n",
    "# =========================\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, filters_1, filters_2, filters_3, dense_units, dropout_1, dropout_2, dropout_3, dropout_dense):\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(1, filters_1, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(filters_1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(dropout_1)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(filters_1, filters_2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(filters_2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(dropout_2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(filters_2, filters_3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(filters_3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Dropout(dropout_3)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(filters_3 * (X_train_scaled.shape[1] // 8), dense_units)\n",
    "        self.dropout_fc = nn.Dropout(dropout_dense)\n",
    "        self.fc2 = nn.Linear(dense_units, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# =========================\n",
    "# Objective function for Optuna\n",
    "# =========================\n",
    "def objective(trial):\n",
    "    filters_1 = trial.suggest_int('filters_1', 64, 128, step=32)\n",
    "    filters_2 = trial.suggest_int('filters_2', 128, 256, step=64)\n",
    "    filters_3 = trial.suggest_int('filters_3', 128, 256, step=64)\n",
    "    dense_units = trial.suggest_int('dense_units', 128, 512, step=128)\n",
    "    dropout_1 = trial.suggest_float('dropout_1', 0.1, 0.3)\n",
    "    dropout_2 = trial.suggest_float('dropout_2', 0.1, 0.3)\n",
    "    dropout_3 = trial.suggest_float('dropout_3', 0.1, 0.3)\n",
    "    dropout_dense = trial.suggest_float('dropout_dense', 0.1, 0.3)\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-3, log=True)\n",
    "\n",
    "    model = CNN1D(filters_1, filters_2, filters_3, dense_units, dropout_1, dropout_2, dropout_3, dropout_dense)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    for epoch in range(30):  # ছোট রাখছি tuning-এর জন্য\n",
    "        model.train()\n",
    "        for xb, yb in loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(X_test_tensor).numpy().flatten()\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    return rmse\n",
    "\n",
    "# =========================\n",
    "# Run Bayesian Tuning\n",
    "# =========================\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=15)\n",
    "\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "for key, val in study.best_params.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "\n",
    "# =========================\n",
    "# Final Model Training\n",
    "# =========================\n",
    "best_params = study.best_params\n",
    "model = CNN1D(**{k: best_params[k] for k in best_params if k != 'lr'})\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_params['lr'])\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    train_losses.append(running_loss / len(loader))\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"cnn1d_model_pytorch.pth\")\n",
    "print(\" Model saved as cnn1d_model_pytorch.pth\")\n",
    "\n",
    "# =========================\n",
    "# Evaluation\n",
    "# =========================\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_train = model(X_train_tensor).numpy().flatten()\n",
    "    y_pred_test = model(X_test_tensor).numpy().flatten()\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rpd = np.std(y_true, ddof=1) / rmse\n",
    "    return r2, rmse, rpd\n",
    "\n",
    "train_r2, train_rmse, train_rpd = evaluate(y_train, y_pred_train)\n",
    "test_r2, test_rmse, test_rpd = evaluate(y_test, y_pred_test)\n",
    "\n",
    "print(\"\\n PyTorch CNN Model Evaluation:\")\n",
    "print(f\"Train R²   : {train_r2:.4f}\")\n",
    "print(f\"Test R²    : {test_r2:.4f}\")\n",
    "print(f\"Train RMSE : {train_rmse:.4f}\")\n",
    "print(f\"Test RMSE  : {test_rmse:.4f}\")\n",
    "print(f\"Train RPD  : {train_rpd:.4f}\")\n",
    "print(f\"Test RPD   : {test_rpd:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# Visualization\n",
    "# =========================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14,12))\n",
    "\n",
    "# 1. Train scatter\n",
    "axes[0,0].scatter(y_train, y_pred_train, color='blue', alpha=0.6)\n",
    "axes[0,0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'g--')\n",
    "axes[0,0].set_title(\"Training: Predicted vs Actual\")\n",
    "\n",
    "# 2. Test scatter\n",
    "axes[0,1].scatter(y_test, y_pred_test, color='red', alpha=0.6)\n",
    "axes[0,1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'g--')\n",
    "axes[0,1].set_title(\"Testing: Predicted vs Actual\")\n",
    "\n",
    "# 3. Loss curve\n",
    "axes[1,0].plot(train_losses, label='Training Loss')\n",
    "axes[1,0].set_title(\"Training Loss Curve\")\n",
    "axes[1,0].legend()\n",
    "\n",
    "# 4. Confusion Matrix\n",
    "y_test_class = (y_test >= np.median(y_test)).astype(int)\n",
    "y_pred_class = (y_pred_test >= np.median(y_test)).astype(int)\n",
    "cm = confusion_matrix(y_test_class, y_pred_class)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=[\"Low\", \"High\"])\n",
    "disp.plot(ax=axes[1,1], cmap='Blues', values_format='d')\n",
    "axes[1,1].set_title(\"Confusion Matrix\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a569ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# CNN-1D (Convolutional Neural Network – One Dimensional) + Bayesian HPO\n",
    "# ===============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import keras_tuner as kt  # <-- Bayesian tuner\n",
    "\n",
    "# -------------------------\n",
    "# Load CSV files containing KS-split data\n",
    "# -------------------------\n",
    "train_path = r\"E:/Thesis 2025/Zohurul/Hyper leaf dataset/train_ks_2.csv\"\n",
    "test_path  = r\"E:/Thesis 2025/Zohurul/Hyper leaf dataset/test_ks_2.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df  = pd.read_csv(test_path)\n",
    "\n",
    "# -------------------------\n",
    "# Define features (X) and target (y)\n",
    "# -------------------------\n",
    "target_column = \"Yield\"\n",
    "X_train = train_df.drop(columns=[target_column]).values\n",
    "y_train = train_df[target_column].values\n",
    "X_test  = test_df.drop(columns=[target_column]).values\n",
    "y_test  = test_df[target_column].values\n",
    "\n",
    "# -------------------------\n",
    "# Standardize features\n",
    "# -------------------------\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled  = scaler_X.transform(X_test)\n",
    "\n",
    "with open(\"scaler_X.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler_X, f)\n",
    "\n",
    "# Reshape for 1D-CNN input: (samples, timesteps, channels=1)\n",
    "X_train_scaled = X_train_scaled[..., np.newaxis]\n",
    "X_test_scaled  = X_test_scaled[...,  np.newaxis]\n",
    "\n",
    "# -------------------------\n",
    "# Model builder for Keras Tuner (Bayesian Optimization)\n",
    "# -------------------------\n",
    "def build_cnn_model(hp: kt.HyperParameters):\n",
    "    model = Sequential()\n",
    "    # Block 1\n",
    "    model.add(Conv1D(\n",
    "        filters=hp.Int('filters_1', min_value=32, max_value=128, step=32),\n",
    "        kernel_size=hp.Choice('kernel_size_1', values=[3,5,7]),\n",
    "        activation='relu',\n",
    "        input_shape=(X_train_scaled.shape[1], 1)\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Dropout(hp.Float('dropout_1', 0.1, 0.5, step=0.1)))\n",
    "\n",
    "    # Block 2\n",
    "    model.add(Conv1D(\n",
    "        filters=hp.Int('filters_2', min_value=64, max_value=256, step=64),\n",
    "        kernel_size=hp.Choice('kernel_size_2', values=[3,5]),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Dropout(hp.Float('dropout_2', 0.1, 0.5, step=0.1)))\n",
    "\n",
    "    # Head\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(\n",
    "        units=hp.Int('dense_units', min_value=64, max_value=256, step=64),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(Dropout(hp.Float('dropout_dense', 0.1, 0.5, step=0.1)))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    lr = hp.Choice('lr', values=[1e-3, 5e-4, 1e-4])\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# Run Bayesian Optimization\n",
    "# -------------------------\n",
    "tuner = kt.BayesianOptimization(\n",
    "    hypermodel=build_cnn_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=20,                 # increase for a deeper search\n",
    "    executions_per_trial=1,\n",
    "    directory='cnn1d_bayes',\n",
    "    project_name='leaf_yield_ks2'\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)\n",
    "reduce_lr  = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=6, min_lr=1e-6, verbose=1)\n",
    "\n",
    "# You can also tune batch size:\n",
    "tune_batch_sizes = [16, 32, 64]\n",
    "\n",
    "print(\"\\n Running Bayesian hyperparameter search...\")\n",
    "tuner.search(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=120,\n",
    "    validation_split=0.2,\n",
    "    batch_size=kt.HyperParameters().Choice('batch_size', tune_batch_sizes) if hasattr(kt, 'HyperParameters') else 32,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"\\n Best hyperparameters:\")\n",
    "for k, v in best_hp.values.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# -------------------------\n",
    "# Build & train best model\n",
    "# -------------------------\n",
    "best_model = tuner.hypermodel.build(best_hp)\n",
    "\n",
    "# If you want to use the tuned batch size (if present), else default to 32\n",
    "best_bs = best_hp.get('batch_size') if 'batch_size' in best_hp.values else 32\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=200,\n",
    "    validation_split=0.2,\n",
    "    batch_size=best_bs,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save final trained model\n",
    "best_model.save(\"CNN1D_final_model.keras\")\n",
    "print(\" Model saved as CNN1D_final_model.keras\")\n",
    "\n",
    "# ---- ONLY ADDITION: also save a pickle alongside (.pkl) ----\n",
    "model_pickle = {\n",
    "    \"model_json\": best_model.to_json(),\n",
    "    \"weights\": best_model.get_weights()\n",
    "}\n",
    "with open(\"CNN1D_final_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_pickle, f)\n",
    "print(\" Also saved pickle: CNN1D_final_model.pkl\")\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# -------------------------\n",
    "# Reload & evaluate\n",
    "# -------------------------\n",
    "model_loaded = load_model(\"CNN1D_final_model.keras\")\n",
    "scaler_loaded = pickle.load(open(\"scaler_X.pkl\", \"rb\"))\n",
    "\n",
    "X_train_scaled_loaded = scaler_loaded.transform(X_train)[..., np.newaxis]\n",
    "X_test_scaled_loaded  = scaler_loaded.transform(X_test)[..., np.newaxis]\n",
    "\n",
    "y_pred_train = model_loaded.predict(X_train_scaled_loaded, verbose=0).flatten()\n",
    "y_pred_test  = model_loaded.predict(X_test_scaled_loaded,  verbose=0).flatten()\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    rpd  = np.std(y_true, ddof=1) / rmse\n",
    "    return r2, rmse, rpd\n",
    "\n",
    "train_r2, train_rmse, train_rpd = evaluate(y_train, y_pred_train)\n",
    "test_r2,  test_rmse,  test_rpd  = evaluate(y_test,  y_pred_test)\n",
    "\n",
    "print(\"\\n Evaluation:\")\n",
    "print(f\"Train R²   : {train_r2:.4f}\")\n",
    "print(f\"Test  R²   : {test_r2:.4f}\")\n",
    "print(f\"Train RMSE : {train_rmse:.4f}\")\n",
    "print(f\"Test  RMSE : {test_rmse:.4f}\")\n",
    "print(f\"Train RPD  : {train_rpd:.4f}\")\n",
    "print(f\"Test  RPD  : {test_rpd:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Plots (same as before)\n",
    "# -------------------------\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14,12))\n",
    "\n",
    "# 1. Train scatter\n",
    "axes[0,0].scatter(y_train, y_pred_train, color='blue', alpha=0.6, label='Train Predicted')\n",
    "z_train = np.polyfit(y_train, y_pred_train, 1); p_train = np.poly1d(z_train)\n",
    "axes[0,0].plot(y_train, p_train(y_train), \"blue\", linestyle='--', label=\"Train Best Fit\")\n",
    "axes[0,0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \"green\", linestyle='-', label=\"Ideal Fit\")\n",
    "axes[0,0].set_title(\"Training Set: Predicted vs Actual\")\n",
    "axes[0,0].set_xlabel(\"Actual Values\"); axes[0,0].set_ylabel(\"Predicted Values\")\n",
    "axes[0,0].legend(); axes[0,0].grid(True)\n",
    "\n",
    "# 2. Test scatter\n",
    "axes[0,1].scatter(y_test, y_pred_test, color='red', alpha=0.6, label='Test Predicted')\n",
    "z_test = np.polyfit(y_test, y_pred_test, 1); p_test = np.poly1d(z_test)\n",
    "axes[0,1].plot(y_test, p_test(y_test), \"orange\", linestyle='--', label=\"Test Best Fit\")\n",
    "axes[0,1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"green\", linestyle='-', label=\"Ideal Fit\")\n",
    "axes[0,1].set_title(\"Test Set: Predicted vs Actual\")\n",
    "axes[0,1].set_xlabel(\"Actual Values\"); axes[0,1].set_ylabel(\"Predicted Values\")\n",
    "axes[0,1].legend(); axes[0,1].grid(True)\n",
    "\n",
    "# 3. Loss curves\n",
    "axes[1,0].plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "axes[1,0].plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
    "axes[1,0].set_title(\"1D-CNN Training & Validation Loss\")\n",
    "axes[1,0].set_xlabel(\"Epochs\"); axes[1,0].set_ylabel(\"MSE Loss\")\n",
    "axes[1,0].legend(); axes[1,0].grid(True)\n",
    "\n",
    "# 4. Confusion-style view (median split)\n",
    "y_test_class = (y_test >= np.median(y_test)).astype(int)\n",
    "y_pred_test_class = (y_pred_test >= np.median(y_test)).astype(int)\n",
    "cm = confusion_matrix(y_test_class, y_pred_test_class)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Low Yield\", \"High Yield\"])\n",
    "disp.plot(ax=axes[1,1], cmap=plt.cm.Blues, values_format=\"d\")\n",
    "axes[1,1].set_title(\"Confusion Matrix\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b8e7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# CNN-1D (Convolutional Neural Network – One Dimensional) + Bayesian HPO\n",
    "# ===============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from tensorflow.keras.models import Sequential, load_model, model_from_json\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import keras_tuner as kt\n",
    "import shap\n",
    "\n",
    "# -------------------------\n",
    "# Load CSV files\n",
    "# -------------------------\n",
    "train_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/train_ks_2.csv\"\n",
    "test_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/test_ks_2.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df  = pd.read_csv(test_path)\n",
    "\n",
    "# -------------------------\n",
    "# Define features (X) and target (y)\n",
    "# -------------------------\n",
    "target_column = \"Yield\"\n",
    "X_train = train_df.drop(columns=[target_column]).values\n",
    "y_train = train_df[target_column].values\n",
    "X_test  = test_df.drop(columns=[target_column]).values\n",
    "y_test  = test_df[target_column].values\n",
    "\n",
    "# -------------------------\n",
    "# Standardize features\n",
    "# -------------------------\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled  = scaler_X.transform(X_test)\n",
    "\n",
    "with open(\"scaler_X.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler_X, f)\n",
    "\n",
    "# Reshape for 1D-CNN input\n",
    "X_train_scaled = X_train_scaled[..., np.newaxis]\n",
    "X_test_scaled  = X_test_scaled[...,  np.newaxis]\n",
    "\n",
    "# -------------------------\n",
    "# Model builder for Keras Tuner\n",
    "# -------------------------\n",
    "def build_cnn_model(hp: kt.HyperParameters):\n",
    "    model = Sequential()\n",
    "    # Block 1\n",
    "    model.add(Conv1D(\n",
    "        filters=hp.Int('filters_1', min_value=32, max_value=128, step=32),\n",
    "        kernel_size=hp.Choice('kernel_size_1', values=[3,5,7]),\n",
    "        activation='relu',\n",
    "        input_shape=(X_train_scaled.shape[1], 1)\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Dropout(hp.Float('dropout_1', 0.1, 0.5, step=0.1)))\n",
    "\n",
    "    # Block 2\n",
    "    model.add(Conv1D(\n",
    "        filters=hp.Int('filters_2', min_value=64, max_value=256, step=64),\n",
    "        kernel_size=hp.Choice('kernel_size_2', values=[3,5]),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Dropout(hp.Float('dropout_2', 0.1, 0.5, step=0.1)))\n",
    "\n",
    "    # Head\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(\n",
    "        units=hp.Int('dense_units', min_value=64, max_value=512, step=64),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(Dropout(hp.Float('dropout_dense', 0.1, 0.5, step=0.1)))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "\n",
    "    lr = hp.Choice('lr', values=[1e-3, 5e-4, 1e-4])\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# Run Bayesian Optimization\n",
    "# -------------------------\n",
    "tuner = kt.BayesianOptimization(\n",
    "    hypermodel=build_cnn_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    directory='cnn1d_bayes',\n",
    "    project_name='leaf_yield_ks2'\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)\n",
    "reduce_lr  = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=6, min_lr=1e-6, verbose=1)\n",
    "\n",
    "tune_batch_sizes = [16, 32, 64]\n",
    "\n",
    "print(\"\\nRunning Bayesian hyperparameter search...\")\n",
    "tuner.search(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=120,\n",
    "    validation_split=0.2,\n",
    "    batch_size=kt.HyperParameters().Choice('batch_size', tune_batch_sizes) if hasattr(kt, 'HyperParameters') else 32,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "for k, v in best_hp.values.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# -------------------------\n",
    "# Build & train best model\n",
    "# -------------------------\n",
    "best_model = tuner.hypermodel.build(best_hp)\n",
    "best_bs = best_hp.get('batch_size') if 'batch_size' in best_hp.values else 32\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=200,\n",
    "    validation_split=0.2,\n",
    "    batch_size=best_bs,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Save models\n",
    "# -------------------------\n",
    "# Save as .keras\n",
    "best_model.save(\"CNN1D_final_model.keras\")\n",
    "\n",
    "# Save as .pkl\n",
    "model_pickle = {\n",
    "    \"model_json\": best_model.to_json(),\n",
    "    \"weights\": best_model.get_weights()\n",
    "}\n",
    "with open(\"CNN1D_final_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_pickle, f)\n",
    "print(\"✅ Models saved (.keras and .pkl)\")\n",
    "\n",
    "# -------------------------\n",
    "# Reload & evaluate from .pkl\n",
    "# -------------------------\n",
    "with open(\"CNN1D_final_model.pkl\", \"rb\") as f:\n",
    "    model_data = pickle.load(f)\n",
    "\n",
    "model_loaded = model_from_json(model_data[\"model_json\"])\n",
    "model_loaded.set_weights(model_data[\"weights\"])\n",
    "model_loaded.compile(optimizer=Adam(), loss='mse', metrics=['mae'])\n",
    "\n",
    "scaler_loaded = pickle.load(open(\"scaler_X.pkl\", \"rb\"))\n",
    "\n",
    "X_train_scaled_loaded = scaler_loaded.transform(X_train)[..., np.newaxis]\n",
    "X_test_scaled_loaded  = scaler_loaded.transform(X_test)[..., np.newaxis]\n",
    "\n",
    "y_pred_train = model_loaded.predict(X_train_scaled_loaded, verbose=0).flatten()\n",
    "y_pred_test  = model_loaded.predict(X_test_scaled_loaded,  verbose=0).flatten()\n",
    "\n",
    "# -------------------------\n",
    "# Evaluation\n",
    "# -------------------------\n",
    "def evaluate(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    rpd  = np.std(y_true, ddof=1) / rmse\n",
    "    return r2, rmse, rpd\n",
    "\n",
    "train_r2, train_rmse, train_rpd = evaluate(y_train, y_pred_train)\n",
    "test_r2,  test_rmse,  test_rpd  = evaluate(y_test,  y_pred_test)\n",
    "\n",
    "print(\"\\nEvaluation:\")\n",
    "print(f\"Train R²   : {train_r2:.4f}\")\n",
    "print(f\"Test  R²   : {test_r2:.4f}\")\n",
    "print(f\"Train RMSE : {train_rmse:.4f}\")\n",
    "print(f\"Test  RMSE : {test_rmse:.4f}\")\n",
    "print(f\"Train RPD  : {train_rpd:.4f}\")\n",
    "print(f\"Test  RPD  : {test_rpd:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Plots: scatter, loss, confusion\n",
    "# -------------------------\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14,12))\n",
    "\n",
    "# Train scatter\n",
    "axes[0,0].scatter(y_train, y_pred_train, color='blue', alpha=0.6, label='Train Predicted')\n",
    "z_train = np.polyfit(y_train, y_pred_train, 1); p_train = np.poly1d(z_train)\n",
    "axes[0,0].plot(y_train, p_train(y_train), \"blue\", linestyle='--', label=\"Train Best Fit\")\n",
    "axes[0,0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \"green\", linestyle='-', label=\"Ideal Fit\")\n",
    "axes[0,0].set_title(\"Training Set: Predicted vs Actual\")\n",
    "axes[0,0].set_xlabel(\"Actual Values\"); axes[0,0].set_ylabel(\"Predicted Values\")\n",
    "axes[0,0].legend(); axes[0,0].grid(True)\n",
    "\n",
    "# Test scatter\n",
    "axes[0,1].scatter(y_test, y_pred_test, color='red', alpha=0.6, label='Test Predicted')\n",
    "z_test = np.polyfit(y_test, y_pred_test, 1); p_test = np.poly1d(z_test)\n",
    "axes[0,1].plot(y_test, p_test(y_test), \"orange\", linestyle='--', label=\"Test Best Fit\")\n",
    "axes[0,1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"green\", linestyle='-', label=\"Ideal Fit\")\n",
    "axes[0,1].set_title(\"Test Set: Predicted vs Actual\")\n",
    "axes[0,1].set_xlabel(\"Actual Values\"); axes[0,1].set_ylabel(\"Predicted Values\")\n",
    "axes[0,1].legend(); axes[0,1].grid(True)\n",
    "\n",
    "# Loss curves\n",
    "axes[1,0].plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "axes[1,0].plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
    "axes[1,0].set_title(\"1D-CNN Training & Validation Loss\")\n",
    "axes[1,0].set_xlabel(\"Epochs\"); axes[1,0].set_ylabel(\"MSE Loss\")\n",
    "axes[1,0].legend(); axes[1,0].grid(True)\n",
    "\n",
    "# Confusion-style (median split)\n",
    "y_test_class = (y_test >= np.median(y_test)).astype(int)\n",
    "y_pred_test_class = (y_pred_test >= np.median(y_test)).astype(int)\n",
    "cm = confusion_matrix(y_test_class, y_pred_test_class)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Low Yield\", \"High Yield\"])\n",
    "disp.plot(ax=axes[1,1], cmap=plt.cm.Blues, values_format=\"d\")\n",
    "axes[1,1].set_title(\"Confusion Matrix\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===============================================================\n",
    "# SHAP: Explainable AI — Top 10 Features with proper coloring\n",
    "# ===============================================================\n",
    "num_bands = X_train.shape[1]\n",
    "wavelength_start = 400\n",
    "wavelength_end   = 1000\n",
    "feature_names = np.linspace(wavelength_start, wavelength_end, num_bands, dtype=int)\n",
    "feature_names = np.array([f\"{wl} nm\" for wl in feature_names])\n",
    "print(f\"✅ Wavelength feature names: {len(feature_names)} bands ({feature_names[0]} → {feature_names[-1]})\")\n",
    "\n",
    "# Background & explain samples\n",
    "bg_n = min(100, len(X_train_scaled_loaded))\n",
    "bg_idx = np.random.choice(len(X_train_scaled_loaded), size=bg_n, replace=False)\n",
    "X_bg = X_train_scaled_loaded[bg_idx]\n",
    "\n",
    "ex_n = min(200, len(X_test_scaled_loaded))\n",
    "ex_idx = np.random.choice(len(X_test_scaled_loaded), size=ex_n, replace=False)\n",
    "X_explain = X_test_scaled_loaded[ex_idx]\n",
    "\n",
    "# Explainer\n",
    "try:\n",
    "    explainer = shap.DeepExplainer(model_loaded, X_bg)\n",
    "    shap_values = explainer.shap_values(X_explain)\n",
    "    if isinstance(shap_values, list): shap_values = shap_values[0]\n",
    "except Exception as e:\n",
    "    print(\"DeepExplainer failed, falling back to KernelExplainer...\", e)\n",
    "    f = lambda x: model_loaded.predict(x.reshape(x.shape[0], x.shape[1], 1)).flatten()\n",
    "    explainer = shap.KernelExplainer(f, X_bg[..., 0])\n",
    "    shap_values = explainer.shap_values(X_explain[..., 0])\n",
    "    shap_values = np.array(shap_values).T\n",
    "\n",
    "shap_values = np.squeeze(shap_values)\n",
    "X_explain_unscaled = scaler_loaded.inverse_transform(np.squeeze(X_explain[...,0]))\n",
    "\n",
    "# Top-10 features\n",
    "mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
    "top_k = 10\n",
    "top_indices = np.argsort(mean_abs_shap)[-top_k:][::-1]\n",
    "top_feature_names = feature_names[top_indices]\n",
    "\n",
    "shap_plot_values = shap_values[:, top_indices].reshape(ex_n, -1)\n",
    "X_plot = X_explain_unscaled[:, top_indices].reshape(ex_n, -1)\n",
    "X_plot_df = pd.DataFrame(X_plot, columns=top_feature_names)\n",
    "print(f\"✅ X_plot_df shape: {X_plot_df.shape}, shap_plot_values shape: {shap_plot_values.shape}\")\n",
    "\n",
    "# -----------------------------\n",
    "# SHAP Summary Plot with bold High/Low\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(12,6))\n",
    "shap.summary_plot(\n",
    "    shap_plot_values,\n",
    "    X_plot_df,\n",
    "    feature_names=top_feature_names,\n",
    "    show=False,\n",
    "    color=X_plot_df.values,\n",
    "    max_display=top_k\n",
    ")\n",
    "plt.title(\"SHAP Summary Plot (1D-CNN) — Top 10 Features\", fontsize=14, fontweight='bold')\n",
    "plt.xticks(fontsize=12, fontweight='bold')\n",
    "plt.yticks(fontsize=12, fontweight='bold')\n",
    "plt.xlabel(\"SHAP value (impact on model output)\", fontsize=12, fontweight='bold')\n",
    "\n",
    "# Customize colorbar for High/Low bold\n",
    "cbar = plt.gcf().axes[-1]\n",
    "cbar.set_ylabel(\"Feature value\", fontsize=12, fontweight='bold')\n",
    "cbar.tick_params(labelsize=12)\n",
    "# High/Low labels\n",
    "cbar.set_yticks([0,1])\n",
    "cbar.set_yticklabels([\"Low\", \"High\"], fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# SHAP Feature Importance Bar Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "shap.summary_plot(\n",
    "    shap_plot_values,\n",
    "    X_plot_df,\n",
    "    feature_names=top_feature_names,\n",
    "    plot_type=\"bar\",\n",
    "    show=False,\n",
    "    max_display=top_k\n",
    ")\n",
    "plt.xlabel(\"Mean SHAP Value\", fontsize=12, fontweight='bold')\n",
    "plt.ylabel(\"Features\", fontsize=12, fontweight='bold')\n",
    "plt.title(\"SHAP Feature Importance (1D-CNN) — Top 10 Features\", fontsize=14, fontweight='bold')\n",
    "plt.xticks(fontsize=12, fontweight='bold')\n",
    "plt.yticks(fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc1e016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# CNN-1D (Convolutional Neural Network – One Dimensional) + Bayesian HPO\n",
    "# ===============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from tensorflow.keras.models import Sequential, load_model, model_from_json\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import pickle\n",
    "import keras_tuner as kt\n",
    "import shap\n",
    "\n",
    "# -------------------------\n",
    "# Load CSV files\n",
    "# -------------------------\n",
    "train_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/train_ks_2.csv\"\n",
    "test_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/test_ks_2.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df  = pd.read_csv(test_path)\n",
    "\n",
    "# -------------------------\n",
    "# Define features (X) and target (y)\n",
    "# -------------------------\n",
    "target_column = \"Yield\"\n",
    "X_train = train_df.drop(columns=[target_column]).values\n",
    "y_train = train_df[target_column].values\n",
    "X_test  = test_df.drop(columns=[target_column]).values\n",
    "y_test  = test_df[target_column].values\n",
    "\n",
    "# -------------------------\n",
    "# Optional: Remove outliers from y_train\n",
    "# -------------------------\n",
    "from scipy.stats import zscore\n",
    "mask = np.abs(zscore(y_train)) < 3\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "# -------------------------\n",
    "# Standardize features using RobustScaler\n",
    "# -------------------------\n",
    "scaler_X = RobustScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled  = scaler_X.transform(X_test)\n",
    "\n",
    "with open(\"scaler_X.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler_X, f)\n",
    "\n",
    "# Reshape for 1D-CNN input\n",
    "X_train_scaled = X_train_scaled[..., np.newaxis]\n",
    "X_test_scaled  = X_test_scaled[...,  np.newaxis]\n",
    "\n",
    "# -------------------------\n",
    "# Model builder for Keras Tuner\n",
    "# -------------------------\n",
    "def build_cnn_model(hp: kt.HyperParameters):\n",
    "    model = Sequential()\n",
    "    # Block 1\n",
    "    model.add(Conv1D(\n",
    "        filters=hp.Int('filters_1', min_value=32, max_value=128, step=32),\n",
    "        kernel_size=hp.Choice('kernel_size_1', values=[3,5,7]),\n",
    "        activation='relu',\n",
    "        input_shape=(X_train_scaled.shape[1], 1)\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Dropout(hp.Float('dropout_1', 0.2, 0.5, step=0.1)))\n",
    "\n",
    "    # Block 2\n",
    "    model.add(Conv1D(\n",
    "        filters=hp.Int('filters_2', min_value=64, max_value=256, step=64),\n",
    "        kernel_size=hp.Choice('kernel_size_2', values=[3,5]),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Dropout(hp.Float('dropout_2', 0.2, 0.5, step=0.1)))\n",
    "\n",
    "    # Head\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(\n",
    "        units=hp.Int('dense_units', min_value=64, max_value=512, step=64),\n",
    "        activation='relu',\n",
    "        kernel_regularizer=l2(0.01)\n",
    "    ))\n",
    "    model.add(Dropout(hp.Float('dropout_dense', 0.2, 0.5, step=0.1)))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    lr = hp.Choice('lr', values=[1e-3, 5e-4, 1e-4, 5e-5])\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# Run Bayesian Optimization\n",
    "# -------------------------\n",
    "tuner = kt.BayesianOptimization(\n",
    "    hypermodel=build_cnn_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='cnn1d_bayes',\n",
    "    project_name='leaf_yield_ks2'\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True, verbose=1)\n",
    "reduce_lr  = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6, verbose=1)\n",
    "\n",
    "tune_batch_sizes = [16,32]\n",
    "\n",
    "print(\"\\nRunning Bayesian hyperparameter search...\")\n",
    "tuner.search(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    batch_size=kt.HyperParameters().Choice('batch_size', tune_batch_sizes) if hasattr(kt, 'HyperParameters') else 32,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "for k, v in best_hp.values.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# -------------------------\n",
    "# Build & train best model\n",
    "# -------------------------\n",
    "best_model = tuner.hypermodel.build(best_hp)\n",
    "best_bs = best_hp.get('batch_size') if 'batch_size' in best_hp.values else 32\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    batch_size=best_bs,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Save models\n",
    "# -------------------------\n",
    "best_model.save(\"CNN1D_final_model.keras\")\n",
    "\n",
    "model_pickle = {\n",
    "    \"model_json\": best_model.to_json(),\n",
    "    \"weights\": best_model.get_weights()\n",
    "}\n",
    "with open(\"CNN1D_final_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_pickle, f)\n",
    "print(\"✅ Models saved (.keras and .pkl)\")\n",
    "\n",
    "# -------------------------\n",
    "# Reload & evaluate from .pkl\n",
    "# -------------------------\n",
    "with open(\"CNN1D_final_model.pkl\", \"rb\") as f:\n",
    "    model_data = pickle.load(f)\n",
    "\n",
    "model_loaded = model_from_json(model_data[\"model_json\"])\n",
    "model_loaded.set_weights(model_data[\"weights\"])\n",
    "model_loaded.compile(optimizer=Adam(), loss='mse', metrics=['mae'])\n",
    "\n",
    "scaler_loaded = pickle.load(open(\"scaler_X.pkl\", \"rb\"))\n",
    "\n",
    "X_train_scaled_loaded = scaler_loaded.transform(X_train)[..., np.newaxis]\n",
    "X_test_scaled_loaded  = scaler_loaded.transform(X_test)[..., np.newaxis]\n",
    "\n",
    "y_pred_train = model_loaded.predict(X_train_scaled_loaded, verbose=0).flatten()\n",
    "y_pred_test  = model_loaded.predict(X_test_scaled_loaded,  verbose=0).flatten()\n",
    "\n",
    "# -------------------------\n",
    "# Evaluation\n",
    "# -------------------------\n",
    "def evaluate(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    rpd  = np.std(y_true, ddof=1) / rmse\n",
    "    return r2, rmse, rpd\n",
    "\n",
    "train_r2, train_rmse, train_rpd = evaluate(y_train, y_pred_train)\n",
    "test_r2,  test_rmse,  test_rpd  = evaluate(y_test,  y_pred_test)\n",
    "\n",
    "print(\"\\nEvaluation:\")\n",
    "print(f\"Train R²   : {train_r2:.4f}\")\n",
    "print(f\"Test  R²   : {test_r2:.4f}\")\n",
    "print(f\"Train RMSE : {train_rmse:.4f}\")\n",
    "print(f\"Test  RMSE : {test_rmse:.4f}\")\n",
    "print(f\"Train RPD  : {train_rpd:.4f}\")\n",
    "print(f\"Test  RPD  : {test_rpd:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Plots: scatter, loss, confusion\n",
    "# -------------------------\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14,12))\n",
    "\n",
    "# Train scatter\n",
    "axes[0,0].scatter(y_train, y_pred_train, color='blue', alpha=0.6, label='Train Predicted')\n",
    "z_train = np.polyfit(y_train, y_pred_train, 1); p_train = np.poly1d(z_train)\n",
    "axes[0,0].plot(y_train, p_train(y_train), \"blue\", linestyle='--', label=\"Train Best Fit\")\n",
    "axes[0,0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \"green\", linestyle='-', label=\"Ideal Fit\")\n",
    "axes[0,0].set_title(\"Training Set: Predicted vs Actual\")\n",
    "axes[0,0].set_xlabel(\"Actual Values\"); axes[0,0].set_ylabel(\"Predicted Values\")\n",
    "axes[0,0].legend(); axes[0,0].grid(True)\n",
    "\n",
    "# Test scatter\n",
    "axes[0,1].scatter(y_test, y_pred_test, color='red', alpha=0.6, label='Test Predicted')\n",
    "z_test = np.polyfit(y_test, y_pred_test, 1); p_test = np.poly1d(z_test)\n",
    "axes[0,1].plot(y_test, p_test(y_test), \"orange\", linestyle='--', label=\"Test Best Fit\")\n",
    "axes[0,1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"green\", linestyle='-', label=\"Ideal Fit\")\n",
    "axes[0,1].set_title(\"Test Set: Predicted vs Actual\")\n",
    "axes[0,1].set_xlabel(\"Actual Values\"); axes[0,1].set_ylabel(\"Predicted Values\")\n",
    "axes[0,1].legend(); axes[0,1].grid(True)\n",
    "\n",
    "# Loss curves\n",
    "axes[1,0].plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "axes[1,0].plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
    "axes[1,0].set_title(\"1D-CNN Training & Validation Loss\")\n",
    "axes[1,0].set_xlabel(\"Epochs\"); axes[1,0].set_ylabel(\"MSE Loss\")\n",
    "axes[1,0].legend(); axes[1,0].grid(True)\n",
    "\n",
    "# Confusion-style (median split)\n",
    "y_test_class = (y_test >= np.median(y_test)).astype(int)\n",
    "y_pred_test_class = (y_pred_test >= np.median(y_test)).astype(int)\n",
    "cm = confusion_matrix(y_test_class, y_pred_test_class)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Low Yield\", \"High Yield\"])\n",
    "disp.plot(ax=axes[1,1], cmap=plt.cm.Blues, values_format=\"d\")\n",
    "axes[1,1].set_title(\"Confusion Matrix\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===============================================================\n",
    "# SHAP: Explainable AI — Top 10 Features with proper coloring\n",
    "# ===============================================================\n",
    "num_bands = X_train.shape[1]\n",
    "wavelength_start = 400\n",
    "wavelength_end   = 1000\n",
    "feature_names = np.linspace(wavelength_start, wavelength_end, num_bands, dtype=int)\n",
    "feature_names = np.array([f\"{wl} nm\" for wl in feature_names])\n",
    "print(f\"✅ Wavelength feature names: {len(feature_names)} bands ({feature_names[0]} → {feature_names[-1]})\")\n",
    "\n",
    "# Background & explain samples\n",
    "bg_n = min(100, len(X_train_scaled_loaded))\n",
    "bg_idx = np.random.choice(len(X_train_scaled_loaded), size=bg_n, replace=False)\n",
    "X_bg = X_train_scaled_loaded[bg_idx]\n",
    "\n",
    "ex_n = min(200, len(X_test_scaled_loaded))\n",
    "ex_idx = np.random.choice(len(X_test_scaled_loaded), size=ex_n, replace=False)\n",
    "X_explain = X_test_scaled_loaded[ex_idx]\n",
    "\n",
    "# Explainer\n",
    "try:\n",
    "    explainer = shap.DeepExplainer(model_loaded, X_bg)\n",
    "    shap_values = explainer.shap_values(X_explain)\n",
    "    if isinstance(shap_values, list): shap_values = shap_values[0]\n",
    "except Exception as e:\n",
    "    print(\"DeepExplainer failed, falling back to KernelExplainer...\", e)\n",
    "    f = lambda x: model_loaded.predict(x.reshape(x.shape[0], x.shape[1], 1)).flatten()\n",
    "    explainer = shap.KernelExplainer(f, X_bg[..., 0])\n",
    "    shap_values = explainer.shap_values(X_explain[..., 0])\n",
    "    shap_values = np.array(shap_values).T\n",
    "\n",
    "shap_values = np.squeeze(shap_values)\n",
    "X_explain_unscaled = scaler_loaded.inverse_transform(np.squeeze(X_explain[...,0]))\n",
    "\n",
    "# Top-10 features\n",
    "mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
    "top_k = 10\n",
    "top_indices = np.argsort(mean_abs_shap)[-top_k:][::-1]\n",
    "top_feature_names = feature_names[top_indices]\n",
    "\n",
    "shap_plot_values = shap_values[:, top_indices].reshape(ex_n, -1)\n",
    "X_plot = X_explain_unscaled[:, top_indices].reshape(ex_n, -1)\n",
    "X_plot_df = pd.DataFrame(X_plot, columns=top_feature_names)\n",
    "print(f\"✅ X_plot_df shape: {X_plot_df.shape}, shap_plot_values shape: {shap_plot_values.shape}\")\n",
    "\n",
    "# -----------------------------\n",
    "# SHAP Summary Plot with bold High/Low\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(12,6))\n",
    "shap.summary_plot(\n",
    "    shap_plot_values,\n",
    "    X_plot_df,\n",
    "    feature_names=top_feature_names,\n",
    "    show=False,\n",
    "    color=X_plot_df.values,\n",
    "    max_display=top_k\n",
    ")\n",
    "plt.title(\"SHAP Summary Plot (1D-CNN) — Top 10 Features\", fontsize=14, fontweight='bold')\n",
    "plt.xticks(fontsize=12, fontweight='bold')\n",
    "plt.yticks(fontsize=12, fontweight='bold')\n",
    "plt.xlabel(\"SHAP value (impact on model output)\", fontsize=12, fontweight='bold')\n",
    "\n",
    "# Customize colorbar for High/Low bold\n",
    "cbar = plt.gcf().axes[-1]\n",
    "cbar.set_ylabel(\"Feature value\", fontsize=12, fontweight='bold')\n",
    "cbar.tick_params(labelsize=12)\n",
    "# High/Low labels\n",
    "cbar.set_yticks([0,1])\n",
    "cbar.set_yticklabels([\"Low\", \"High\"], fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# SHAP Feature Importance Bar Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "shap.summary_plot(\n",
    "    shap_plot_values,\n",
    "    X_plot_df,\n",
    "    feature_names=top_feature_names,\n",
    "    plot_type=\"bar\",\n",
    "    show=False,\n",
    "    max_display=top_k\n",
    ")\n",
    "plt.xlabel(\"Mean SHAP Value\", fontsize=12, fontweight='bold')\n",
    "plt.ylabel(\"Features\", fontsize=12, fontweight='bold')\n",
    "plt.title(\"SHAP Feature Importance (1D-CNN) — Top 10 Features\", fontsize=14, fontweight='bold')\n",
    "plt.xticks(fontsize=12, fontweight='bold')\n",
    "plt.yticks(fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7929235a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# CNN-1D (Convolutional Neural Network – One Dimensional) + Bayesian HPO\n",
    "# ===============================================================\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from tensorflow.keras.models import Sequential, load_model, model_from_json\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import pickle\n",
    "import keras_tuner as kt\n",
    "import shap\n",
    "import tensorflow as tf\n",
    "\n",
    "# -------------------------\n",
    "# Fix Random Seeds for Reproducibility\n",
    "# -------------------------\n",
    "seed_value = 9 # You can choose any fixed number\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Make TensorFlow deterministic (optional but useful)\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
    "\n",
    "# -------------------------\n",
    "# Load CSV files\n",
    "# -------------------------\n",
    "train_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/train_ks_2.csv\"\n",
    "test_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/test_ks_2.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df  = pd.read_csv(test_path)\n",
    "\n",
    "# -------------------------\n",
    "# Define features (X) and target (y)\n",
    "# -------------------------\n",
    "target_column = \"Yield\"\n",
    "X_train = train_df.drop(columns=[target_column]).values\n",
    "y_train = train_df[target_column].values\n",
    "X_test  = test_df.drop(columns=[target_column]).values\n",
    "y_test  = test_df[target_column].values\n",
    "\n",
    "# -------------------------\n",
    "# Optional: Remove outliers from y_train\n",
    "# -------------------------\n",
    "from scipy.stats import zscore\n",
    "mask = np.abs(zscore(y_train)) < 3\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "# -------------------------\n",
    "# Standardize features using RobustScaler\n",
    "# -------------------------\n",
    "scaler_X = RobustScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled  = scaler_X.transform(X_test)\n",
    "\n",
    "with open(\"scaler_X.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler_X, f)\n",
    "\n",
    "# Reshape for 1D-CNN input\n",
    "X_train_scaled = X_train_scaled[..., np.newaxis]\n",
    "X_test_scaled  = X_test_scaled[...,  np.newaxis]\n",
    "\n",
    "# -------------------------\n",
    "# Model builder for Keras Tuner\n",
    "# -------------------------\n",
    "def build_cnn_model(hp: kt.HyperParameters):\n",
    "    model = Sequential()\n",
    "    # Block 1\n",
    "    model.add(Conv1D(\n",
    "        filters=hp.Int('filters_1', min_value=32, max_value=128, step=32),\n",
    "        kernel_size=hp.Choice('kernel_size_1', values=[3,5,7]),\n",
    "        activation='relu',\n",
    "        input_shape=(X_train_scaled.shape[1], 1)\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Dropout(hp.Float('dropout_1', 0.2, 0.5, step=0.1)))\n",
    "\n",
    "    # Block 2\n",
    "    model.add(Conv1D(\n",
    "        filters=hp.Int('filters_2', min_value=64, max_value=256, step=64),\n",
    "        kernel_size=hp.Choice('kernel_size_2', values=[3,5]),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Dropout(hp.Float('dropout_2', 0.2, 0.5, step=0.1)))\n",
    "\n",
    "    # Head\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(\n",
    "        units=hp.Int('dense_units', min_value=64, max_value=512, step=64),\n",
    "        activation='relu',\n",
    "        kernel_regularizer=l2(0.01)\n",
    "    ))\n",
    "    model.add(Dropout(hp.Float('dropout_dense', 0.2, 0.5, step=0.1)))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    lr = hp.Choice('lr', values=[1e-3, 5e-4, 1e-4, 5e-5])\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# Run Bayesian Optimization\n",
    "# -------------------------\n",
    "tuner = kt.BayesianOptimization(\n",
    "    hypermodel=build_cnn_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='cnn1d_bayes',\n",
    "    project_name='leaf_yield_ks2'\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True, verbose=1)\n",
    "reduce_lr  = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6, verbose=1)\n",
    "\n",
    "tune_batch_sizes = [16,32]\n",
    "\n",
    "print(\"\\nRunning Bayesian hyperparameter search...\")\n",
    "tuner.search(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    batch_size=kt.HyperParameters().Choice('batch_size', tune_batch_sizes) if hasattr(kt, 'HyperParameters') else 32,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "for k, v in best_hp.values.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# -------------------------\n",
    "# Build & train best model\n",
    "# -------------------------\n",
    "best_model = tuner.hypermodel.build(best_hp)\n",
    "best_bs = best_hp.get('batch_size') if 'batch_size' in best_hp.values else 32\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    batch_size=best_bs,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Save models\n",
    "# -------------------------\n",
    "best_model.save(\"CNN1D_final_model.keras\")\n",
    "\n",
    "model_pickle = {\n",
    "    \"model_json\": best_model.to_json(),\n",
    "    \"weights\": best_model.get_weights()\n",
    "}\n",
    "with open(\"CNN1D_final_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_pickle, f)\n",
    "print(\" Models saved (.keras and .pkl)\")\n",
    "\n",
    "# -------------------------\n",
    "# Reload & evaluate from .pkl\n",
    "# -------------------------\n",
    "with open(\"CNN1D_final_model.pkl\", \"rb\") as f:\n",
    "    model_data = pickle.load(f)\n",
    "\n",
    "model_loaded = model_from_json(model_data[\"model_json\"])\n",
    "model_loaded.set_weights(model_data[\"weights\"])\n",
    "model_loaded.compile(optimizer=Adam(), loss='mse', metrics=['mae'])\n",
    "\n",
    "scaler_loaded = pickle.load(open(\"scaler_X.pkl\", \"rb\"))\n",
    "\n",
    "X_train_scaled_loaded = scaler_loaded.transform(X_train)[..., np.newaxis]\n",
    "X_test_scaled_loaded  = scaler_loaded.transform(X_test)[..., np.newaxis]\n",
    "\n",
    "y_pred_train = model_loaded.predict(X_train_scaled_loaded, verbose=0).flatten()\n",
    "y_pred_test  = model_loaded.predict(X_test_scaled_loaded,  verbose=0).flatten()\n",
    "\n",
    "# -------------------------\n",
    "# Evaluation\n",
    "# -------------------------\n",
    "def evaluate(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    rpd  = np.std(y_true, ddof=1) / rmse\n",
    "    return r2, rmse, rpd\n",
    "\n",
    "train_r2, train_rmse, train_rpd = evaluate(y_train, y_pred_train)\n",
    "test_r2,  test_rmse,  test_rpd  = evaluate(y_test,  y_pred_test)\n",
    "\n",
    "print(\"\\nEvaluation:\")\n",
    "print(f\"Train R²   : {train_r2:.4f}\")\n",
    "print(f\"Test  R²   : {test_r2:.4f}\")\n",
    "print(f\"Train RMSE : {train_rmse:.4f}\")\n",
    "print(f\"Test  RMSE : {test_rmse:.4f}\")\n",
    "print(f\"Train RPD  : {train_rpd:.4f}\")\n",
    "print(f\"Test  RPD  : {test_rpd:.4f}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Plots: scatter, loss, confusion (manual bold)\n",
    "# -------------------------\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14,12))\n",
    "\n",
    "# -------------------------\n",
    "# Train scatter\n",
    "# -------------------------\n",
    "axes[0,0].scatter(y_train, y_pred_train, color='blue', alpha=0.6, label='Train Predicted')\n",
    "z_train = np.polyfit(y_train, y_pred_train, 1); p_train = np.poly1d(z_train)\n",
    "axes[0,0].plot(y_train, p_train(y_train), \"blue\", linestyle='--', label=\"Train Best Fit\")\n",
    "axes[0,0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \"green\", linestyle='-', label=\"Ideal Fit\")\n",
    "axes[0,0].set_title(\"Training Set: Predicted vs Actual\", fontweight='bold', fontsize=14)\n",
    "axes[0,0].set_xlabel(\"Actual Values\", fontweight='bold', fontsize=12)\n",
    "axes[0,0].set_ylabel(\"Predicted Values\", fontweight='bold', fontsize=12)\n",
    "axes[0,0].legend(prop={'weight':'bold', 'size':10})\n",
    "axes[0,0].grid(True)\n",
    "\n",
    "# make tick labels bold\n",
    "for tick in axes[0,0].get_xticklabels() + axes[0,0].get_yticklabels():\n",
    "    tick.set_fontweight('bold')\n",
    "\n",
    "# -------------------------\n",
    "# Test scatter\n",
    "# -------------------------\n",
    "axes[0,1].scatter(y_test, y_pred_test, color='red', alpha=0.6, label='Test Predicted')\n",
    "z_test = np.polyfit(y_test, y_pred_test, 1); p_test = np.poly1d(z_test)\n",
    "axes[0,1].plot(y_test, p_test(y_test), \"orange\", linestyle='--', label=\"Test Best Fit\")\n",
    "axes[0,1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"green\", linestyle='-', label=\"Ideal Fit\")\n",
    "axes[0,1].set_title(\"Test Set: Predicted vs Actual\", fontweight='bold', fontsize=14)\n",
    "axes[0,1].set_xlabel(\"Actual Values\", fontweight='bold', fontsize=12)\n",
    "axes[0,1].set_ylabel(\"Predicted Values\", fontweight='bold', fontsize=12)\n",
    "axes[0,1].legend(prop={'weight':'bold', 'size':10})\n",
    "axes[0,1].grid(True)\n",
    "\n",
    "for tick in axes[0,1].get_xticklabels() + axes[0,1].get_yticklabels():\n",
    "    tick.set_fontweight('bold')\n",
    "\n",
    "# -------------------------\n",
    "# Loss curves\n",
    "# -------------------------\n",
    "axes[1,0].plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "axes[1,0].plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
    "axes[1,0].set_title(\"1D-CNN Training & Validation Loss\", fontweight='bold', fontsize=14)\n",
    "axes[1,0].set_xlabel(\"Epochs\", fontweight='bold', fontsize=12)\n",
    "axes[1,0].set_ylabel(\"MSE Loss\", fontweight='bold', fontsize=12)\n",
    "axes[1,0].legend(prop={'weight':'bold', 'size':10})\n",
    "axes[1,0].grid(True)\n",
    "\n",
    "for tick in axes[1,0].get_xticklabels() + axes[1,0].get_yticklabels():\n",
    "    tick.set_fontweight('bold')\n",
    "\n",
    "# -------------------------\n",
    "# Confusion matrix (median split)\n",
    "# -------------------------\n",
    "y_test_class = (y_test >= np.median(y_test)).astype(int)\n",
    "y_pred_test_class = (y_pred_test >= np.median(y_test)).astype(int)\n",
    "cm = confusion_matrix(y_test_class, y_pred_test_class)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Low Yield\", \"High Yield\"])\n",
    "disp.plot(ax=axes[1,1], cmap=plt.cm.Blues, values_format=\"d\")\n",
    "\n",
    "axes[1,1].set_title(\"Confusion Matrix\", fontweight='bold', fontsize=14)\n",
    "axes[1,1].set_xlabel(\"Predicted Label\", fontweight='bold', fontsize=12)\n",
    "axes[1,1].set_ylabel(\"True Label\", fontweight='bold', fontsize=12)\n",
    "\n",
    "# make tick and text labels bold\n",
    "for tick in axes[1,1].get_xticklabels() + axes[1,1].get_yticklabels():\n",
    "    tick.set_fontweight('bold')\n",
    "\n",
    "for txt in axes[1,1].texts:\n",
    "    txt.set_fontweight('bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# SHAP: Explainable AI — Top 10 Features with proper coloring\n",
    "# ===============================================================\n",
    "num_bands = X_train.shape[1]\n",
    "wavelength_start = 400\n",
    "wavelength_end   = 1000\n",
    "feature_names = np.linspace(wavelength_start, wavelength_end, num_bands, dtype=int)\n",
    "feature_names = np.array([f\"{wl} nm\" for wl in feature_names])\n",
    "print(f\" Wavelength feature names: {len(feature_names)} bands ({feature_names[0]} → {feature_names[-1]})\")\n",
    "\n",
    "# Background & explain samples\n",
    "bg_n = min(100, len(X_train_scaled_loaded))\n",
    "bg_idx = np.random.choice(len(X_train_scaled_loaded), size=bg_n, replace=False)\n",
    "X_bg = X_train_scaled_loaded[bg_idx]\n",
    "\n",
    "ex_n = min(200, len(X_test_scaled_loaded))\n",
    "ex_idx = np.random.choice(len(X_test_scaled_loaded), size=ex_n, replace=False)\n",
    "X_explain = X_test_scaled_loaded[ex_idx]\n",
    "\n",
    "# Explainer\n",
    "try:\n",
    "    explainer = shap.DeepExplainer(model_loaded, X_bg)\n",
    "    shap_values = explainer.shap_values(X_explain)\n",
    "    if isinstance(shap_values, list): shap_values = shap_values[0]\n",
    "except Exception as e:\n",
    "    print(\"DeepExplainer failed, falling back to KernelExplainer...\", e)\n",
    "    f = lambda x: model_loaded.predict(x.reshape(x.shape[0], x.shape[1], 1)).flatten()\n",
    "    explainer = shap.KernelExplainer(f, X_bg[..., 0])\n",
    "    shap_values = explainer.shap_values(X_explain[..., 0])\n",
    "    shap_values = np.array(shap_values).T\n",
    "\n",
    "shap_values = np.squeeze(shap_values)\n",
    "X_explain_unscaled = scaler_loaded.inverse_transform(np.squeeze(X_explain[...,0]))\n",
    "\n",
    "# Top-10 features\n",
    "mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
    "top_k = 10\n",
    "top_indices = np.argsort(mean_abs_shap)[-top_k:][::-1]\n",
    "top_feature_names = feature_names[top_indices]\n",
    "\n",
    "shap_plot_values = shap_values[:, top_indices].reshape(ex_n, -1)\n",
    "X_plot = X_explain_unscaled[:, top_indices].reshape(ex_n, -1)\n",
    "X_plot_df = pd.DataFrame(X_plot, columns=top_feature_names)\n",
    "print(f\" X_plot_df shape: {X_plot_df.shape}, shap_plot_values shape: {shap_plot_values.shape}\")\n",
    "\n",
    "# -----------------------------\n",
    "# SHAP Summary Plot with bold High/Low\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(12,6))\n",
    "shap.summary_plot(\n",
    "    shap_plot_values,\n",
    "    X_plot_df,\n",
    "    feature_names=top_feature_names,\n",
    "    show=False,\n",
    "    color=X_plot_df.values,\n",
    "    max_display=top_k\n",
    ")\n",
    "plt.title(\"SHAP Summary Plot\", fontsize=14, fontweight='bold')\n",
    "plt.ylabel(\"Wavelengths (nm)\", fontsize=12, fontweight='bold')\n",
    "plt.xticks(fontsize=12, fontweight='bold')\n",
    "plt.yticks(fontsize=12, fontweight='bold')\n",
    "plt.xlabel(\"SHAP value (impact on model output)\", fontsize=12, fontweight='bold')\n",
    "\n",
    "# Customize colorbar for High/Low bold\n",
    "cbar = plt.gcf().axes[-1]\n",
    "cbar.set_ylabel(\"Feature value\", fontsize=12, fontweight='bold')\n",
    "cbar.tick_params(labelsize=12)\n",
    "# High/Low labels\n",
    "cbar.set_yticks([0,1])\n",
    "cbar.set_yticklabels([\"Low\", \"High\"], fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# SHAP Feature Importance Bar Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "shap.summary_plot(\n",
    "    shap_plot_values,\n",
    "    X_plot_df,\n",
    "    feature_names=top_feature_names,\n",
    "    plot_type=\"bar\",\n",
    "    show=False,\n",
    "    max_display=top_k\n",
    ")\n",
    "plt.xlabel(\"Mean SHAP Value\", fontsize=12, fontweight='bold')\n",
    "plt.ylabel(\"Wavelengths (nm)\", fontsize=12, fontweight='bold')\n",
    "plt.title(\"SHAP Feature Importance\", fontsize=14, fontweight='bold')\n",
    "plt.xticks(fontsize=12, fontweight='bold')\n",
    "plt.yticks(fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55fbb41",
   "metadata": {},
   "source": [
    "Test Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ae0ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ Load Excel files\n",
    "# -----------------------------\n",
    "# File 1: Only IDs with grain weight\n",
    "grain_df = pd.read_excel(r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/image id.xlsx\")\n",
    "# File 2: Full dataset where you want to remove dissimilar IDs\n",
    "full_df = pd.read_excel(r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/hyperspectral.xlsx\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ Make sure the column name is correct\n",
    "# Replace 'ImageID' with the actual column name in your files\n",
    "# -----------------------------\n",
    "grain_ids = set(grain_df['Image ID'])\n",
    "full_df_filtered = full_df[full_df['Image ID'].isin(grain_ids)]\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ Save the filtered dataset\n",
    "# -----------------------------\n",
    "full_df_filtered.to_excel(r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/full_dataset_filtered.xlsx\", index=False)\n",
    "\n",
    "print(f\"Original full dataset rows: {len(full_df)}\")\n",
    "print(f\"Filtered dataset rows: {len(full_df_filtered)}\")\n",
    "print(\"Dissimilar Image IDs removed and new Excel saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
