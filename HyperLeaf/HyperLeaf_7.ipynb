{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79296312",
   "metadata": {},
   "source": [
    "Section 1(Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0baaa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max Normalization for Hyperspectral Data\n",
    "import pandas as pd                       # For reading/writing Excel/CSV and handling DataFrames\n",
    "from sklearn.preprocessing import MinMaxScaler   # For Min-Max scaling of features\n",
    "\n",
    "#  Load the dataset \n",
    "input_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/hyperspectral.xlsx\"  # Path to Excel file\n",
    "df = pd.read_excel(input_path)            # Read the Excel file into a DataFrame\n",
    "\n",
    "print(\"Original Columns:\", df.columns.tolist())   # Show all column names for verification\n",
    "print(\"Shape before:\", df.shape)                  # Print dataset shape (rows, columns) before preprocessing\n",
    "\n",
    "#  Drop unwanted columns \n",
    "drop_cols = ['Image ID', 'Minimum Reflectance', 'Otsu Threshold']  # Columns to remove\n",
    "df = df.drop(columns=drop_cols, errors='ignore')  # Drop them; ignore errors if any column not present\n",
    "\n",
    "#  Separate target column \n",
    "target_col = 'Yield'                      # This is the target variable\n",
    "y = df[target_col]                         # Store Yield separately (we don’t normalize it)\n",
    "X = df.drop(columns=[target_col])          # All other columns are features to normalize\n",
    "\n",
    "#  Min-Max normalization \n",
    "scaler = MinMaxScaler()                    # Create MinMaxScaler instance (default range: 0–1)\n",
    "X_scaled = scaler.fit_transform(X)         # Fit scaler to features & transform them into scaled array\n",
    "\n",
    "#  Reconstruct DataFrame \n",
    "normalized_df = pd.DataFrame(X_scaled, columns=X.columns)  # Convert scaled array back to DataFrame with original column names\n",
    "normalized_df[target_col] = y                               # Add back the original Yield column (unscaled)\n",
    "\n",
    "print(\"Shape after normalization:\", normalized_df.shape)    # Print new shape after dropping columns and adding scaled features\n",
    "\n",
    "#  Save processed file \n",
    "output_path = \"normalized_minmax_with_target.csv\"           # Output file name (will save in current working directory)\n",
    "normalized_df.to_csv(output_path, index=False)              # Save normalized data to CSV without row index\n",
    "\n",
    "print(f\"✅ Normalized file saved as: {output_path}\")         # Confirmation message after saving\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d4eb68",
   "metadata": {},
   "source": [
    "Section 2(KS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8b1b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data partition using KS (Kennard–Stone) method\n",
    "import pandas as pd                       # For handling CSV files and tabular data\n",
    "import numpy as np                        # For numerical operations\n",
    "from scipy.spatial.distance import cdist  # To compute pairwise Euclidean distances\n",
    "\n",
    "# Load dataset from CSV file\n",
    "input_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/normalized_minmax_with_target.csv\"\n",
    "df = pd.read_csv(input_path)              # Read CSV into pandas DataFrame\n",
    "\n",
    "# Identify columns to keep (not for splitting)\n",
    "keep_cols = ['Image ID', 'Yield', 'Minimum Reflectance', 'Otsu'] # Keep only the columns that actually exist in the dataset \n",
    "\n",
    "keep_cols_existing = [col for col in keep_cols if col in df.columns]  # Select remaining columns (hyperspectral bands) for splitting\n",
    "\n",
    "X_cols = [col for col in df.columns if col not in keep_cols_existing]  \n",
    "X = df[X_cols].values                     # Extract band values as NumPy array for KS algorithm\n",
    "\n",
    "# Define Kennard–Stone algorithm function\n",
    "def kennard_stone(X, n_samples):\n",
    "    n_total = X.shape[0]                              # Total number of samples\n",
    "    dist_matrix = cdist(X, X, metric='euclidean')     # Compute pairwise Euclidean distance matrix\n",
    "    \n",
    "    # Step 1: select two samples farthest apart\n",
    "    i1, i2 = np.unravel_index(np.argmax(dist_matrix, axis=None), dist_matrix.shape)  \n",
    "    selected = [i1, i2]                               # Start with two farthest points\n",
    "    remaining = list(set(range(n_total)) - set(selected))  # Remaining samples to select from\n",
    "    \n",
    "    # Step 2: iteratively select sample farthest from selected set\n",
    "    while len(selected) < n_samples:\n",
    "        # Compute minimum distance of each remaining sample to any selected sample\n",
    "        dist_to_selected = np.min(dist_matrix[remaining][:, selected], axis=1)  \n",
    "        next_idx = remaining[np.argmax(dist_to_selected)]  # Pick the one with maximum distance\n",
    "        selected.append(next_idx)                           # Add to selected list\n",
    "        remaining.remove(next_idx)                          # Remove from remaining\n",
    "    \n",
    "    return selected  # Return list of selected indices for training set\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "n_train = int(0.8 * X.shape[0])   # 80% of samples for training\n",
    "train_idx = kennard_stone(X, n_train)  # Get training indices using KS algorithm\n",
    "test_idx = list(set(range(X.shape[0])) - set(train_idx))  # Remaining samples for testing\n",
    "\n",
    "# Create DataFrames for train and test sets\n",
    "df_train = df.iloc[train_idx].reset_index(drop=True)  # Training set\n",
    "df_test = df.iloc[test_idx].reset_index(drop=True)    # Test set\n",
    "\n",
    "# Save train and test sets to CSV files\n",
    "df_train.to_csv(r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/train_ks.csv\", index=False)\n",
    "df_test.to_csv(r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/test_ks.csv\", index=False)\n",
    "\n",
    "# Print confirmation and dataset shapes\n",
    "print(\"✅ Kennard–Stone split done!\")  \n",
    "print(\"Train set shape:\", df_train.shape)\n",
    "print(\"Test set shape :\", df_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1bcf17",
   "metadata": {},
   "source": [
    "Section 3(PLSR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9c8405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using PLSR (Partial Least Squares Regression) technique\n",
    "import pandas as pd  # For data handling\n",
    "import numpy as np  # For numerical operations\n",
    "from sklearn.cross_decomposition import PLSRegression  # PLS regression model\n",
    "from sklearn.preprocessing import StandardScaler  # Feature scaling\n",
    "from sklearn.metrics import mean_squared_error, r2_score  # Evaluation metrics\n",
    "\n",
    "# Load train and test datasets\n",
    "train_df = pd.read_csv(r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/train_ks.csv\")  # Load training CSV\n",
    "test_df = pd.read_csv(r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/test_ks.csv\")  # Load testing CSV\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)  # Show training data shape\n",
    "print(\"Test shape:\", test_df.shape)  # Show testing data shape\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "target_column = \"Yield\"  # Replace with your actual target column name\n",
    "X_train = train_df.drop(columns=[target_column])  # Features for training\n",
    "y_train = train_df[target_column]  # Target for training\n",
    "\n",
    "X_test = test_df.drop(columns=[target_column])  # Features for testing\n",
    "y_test = test_df[target_column]  # Target for testing\n",
    "\n",
    "# Standardize the feature data\n",
    "scaler = StandardScaler()  # Initialize scaler\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit and transform training features\n",
    "X_test_scaled = scaler.transform(X_test)  # Transform testing features with same scaler\n",
    "\n",
    "# Train PLSR model with up to 10 components\n",
    "n_components = min(X_train.shape[1], 10)  # Choose number of components (max 10 or number of features)\n",
    "pls = PLSRegression(n_components=n_components)  # Initialize PLS model\n",
    "pls.fit(X_train_scaled, y_train)  # Fit PLS model to training data\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = pls.predict(X_train_scaled)  # Predict on training set\n",
    "y_pred_test = pls.predict(X_test_scaled)  # Predict on testing set\n",
    "\n",
    "# Evaluate model performance\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))  # RMSE for training\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))  # RMSE for testing\n",
    "\n",
    "train_r2 = r2_score(y_train, y_pred_train)  # R² for training\n",
    "test_r2 = r2_score(y_test, y_pred_test)  # R² for testing\n",
    "\n",
    "train_rpd = np.std(y_train, ddof=1) / train_rmse  # RPD for training\n",
    "test_rpd = np.std(y_test, ddof=1) / test_rmse  # RPD for testing\n",
    "\n",
    "print(\"\\nPLSR Model Evaluation:\")  # Print evaluation header\n",
    "print(f\"Train R²             : {train_r2:.4f}\")  # Display training R²\n",
    "print(f\"Test R²              : {test_r2:.4f}\")  # Display testing R²\n",
    "print(f\"Train RMSE           : {train_rmse:.4f}\")  # Display training RMSE\n",
    "print(f\"Test RMSE            : {test_rmse:.4f}\")  # Display testing RMSE\n",
    "print(f\"Train RPD            : {train_rpd:.4f}\")  # Display training RPD\n",
    "print(f\"Test RPD             : {test_rpd:.4f}\")  # Display testing RPD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52d9565",
   "metadata": {},
   "source": [
    "Section 4(RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51ca2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using RF (Random Forest) model\n",
    "import pandas as pd  # For data handling\n",
    "import numpy as np  # For numerical operations\n",
    "from sklearn.ensemble import RandomForestRegressor  # Random Forest Regressor\n",
    "from sklearn.model_selection import train_test_split  # Train/test split if needed\n",
    "from sklearn.preprocessing import StandardScaler  # Feature scaling\n",
    "from sklearn.metrics import mean_squared_error, r2_score  # Evaluation metrics\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/train_ks.csv\")  # Load training CSV\n",
    "test_df = pd.read_csv(r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/test_ks.csv\")  # Load testing CSV\n",
    "\n",
    "target_column = \"Yield\"  # Define target column\n",
    "X_train = train_df.drop(columns=[target_column]).values  # Features for training\n",
    "y_train = train_df[target_column].values  # Target for training\n",
    "\n",
    "X_test = test_df.drop(columns=[target_column]).values  # Features for testing\n",
    "y_test = test_df[target_column].values  # Target for testing\n",
    "\n",
    "# Standardize features (optional but recommended for some models)\n",
    "scaler = StandardScaler()  # Initialize scaler\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit and transform training features\n",
    "X_test_scaled = scaler.transform(X_test)  # Transform testing features with same scaler\n",
    "\n",
    "# CPU Random Forest Model\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=500,  # Number of trees\n",
    "    max_depth=30,  # Maximum depth of each tree\n",
    "    max_features='sqrt',  # Max features considered at each split\n",
    "    random_state=42,  # Ensure reproducibility\n",
    "    n_jobs=-1  # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train_scaled, y_train)  # Fit model to training data\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = rf.predict(X_train_scaled)  # Predict on training set\n",
    "y_pred_test = rf.predict(X_test_scaled)  # Predict on testing set\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))  # Compute RMSE\n",
    "    r2 = r2_score(y_true, y_pred)  # Compute R² score\n",
    "    rpd = np.std(y_true, ddof=1) / rmse  # Compute RPD\n",
    "    return r2, rmse, rpd  # Return evaluation metrics\n",
    "\n",
    "train_r2, train_rmse, train_rpd = evaluate(y_train, y_pred_train)  # Evaluate training performance\n",
    "test_r2, test_rmse, test_rpd = evaluate(y_test, y_pred_test)  # Evaluate testing performance\n",
    "\n",
    "print(\"\\nRandom Forest Model Evaluation:\")  # Print evaluation header\n",
    "print(f\"Train R²   : {train_r2:.4f}\")  # Display training R²\n",
    "print(f\"Test R²    : {test_r2:.4f}\")  # Display testing R²\n",
    "print(f\"Train RMSE : {train_rmse:.4f}\")  # Display training RMSE\n",
    "print(f\"Test RMSE  : {test_rmse:.4f}\")  # Display testing RMSE\n",
    "print(f\"Train RPD  : {train_rpd:.4f}\")  # Display training RPD\n",
    "print(f\"Test RPD   : {test_rpd:.4f}\")  # Display testing RPD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb717cdb",
   "metadata": {},
   "source": [
    "Section 5(CNN-1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccceb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN-1D (Convolutional Neural Network – One Dimensional)\n",
    "import pandas as pd                                      # For handling CSV files and tabular data\n",
    "import numpy as np                                       # For numerical operations\n",
    "from sklearn.preprocessing import StandardScaler         # For feature standardization\n",
    "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, ConfusionMatrixDisplay  # Evaluation metrics\n",
    "import matplotlib.pyplot as plt                           # For plotting\n",
    "from tensorflow.keras.models import Sequential, load_model  # Build and load Keras models\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization  # CNN layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping     # Early stopping during training\n",
    "import pickle                                           # For saving and loading scaler objects\n",
    "\n",
    "# Load CSV files containing KS-split data\n",
    "train_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/train_ks.csv\"  # Path to training data\n",
    "test_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/test_ks.csv\"    # Path to test data\n",
    "\n",
    "train_df = pd.read_csv(train_path)                       # Load training data into a DataFrame\n",
    "test_df = pd.read_csv(test_path)                         # Load test data into a DataFrame\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "target_column = \"Yield\"                                  # Target column to predict\n",
    "X_train = train_df.drop(columns=[target_column]).values  # Extract features from training set\n",
    "y_train = train_df[target_column].values                 # Extract target values from training set\n",
    "X_test = test_df.drop(columns=[target_column]).values    # Extract features from test set\n",
    "y_test = test_df[target_column].values                   # Extract target values from test set\n",
    "\n",
    "# Standardize features to zero mean and unit variance\n",
    "scaler_X = StandardScaler()                               # Initialize StandardScaler\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)          # Fit on training data and transform\n",
    "X_test_scaled = scaler_X.transform(X_test)                # Transform test data with same scaler\n",
    "\n",
    "# Save the scaler object for later reuse\n",
    "with open(\"scaler_X.pkl\", \"wb\") as f:                     # Open file to save scaler\n",
    "    pickle.dump(scaler_X, f)                              # Save scaler object\n",
    "\n",
    "# Reshape data for 1D-CNN input: (samples, timesteps, features=1)\n",
    "X_train_scaled = X_train_scaled[..., np.newaxis]          # Add extra dimension for CNN input\n",
    "X_test_scaled = X_test_scaled[..., np.newaxis]            # Add extra dimension for CNN input\n",
    "\n",
    "# Build 1D-CNN model using Keras Sequential API\n",
    "model = Sequential([                                      # Initialize sequential model\n",
    "    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train_scaled.shape[1],1)),  # Conv layer with 64 filters\n",
    "    BatchNormalization(),                                 # Normalize activations\n",
    "    MaxPooling1D(pool_size=2),                            # Downsample by factor of 2\n",
    "    Dropout(0.2),                                         # Dropout for regularization\n",
    "\n",
    "    Conv1D(128, kernel_size=3, activation='relu'),       # Second Conv layer with 128 filters\n",
    "    BatchNormalization(),                                 # Normalize activations\n",
    "    MaxPooling1D(pool_size=2),                            # Downsample\n",
    "    Dropout(0.2),                                         # Dropout\n",
    "\n",
    "    Flatten(),                                            # Flatten output to feed Dense layers\n",
    "    Dense(128, activation='relu'),                        # Fully connected Dense layer\n",
    "    Dropout(0.2),                                         # Dropout\n",
    "    Dense(1, activation='linear')                         # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile model with optimizer and loss function\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])  # Use Adam optimizer and MSE loss\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)  # Stop if val_loss doesn't improve\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,           # Training features and target\n",
    "    epochs=100,                        # Maximum number of epochs\n",
    "    batch_size=32,                     # Batch size\n",
    "    validation_split=0.2,              # Use 20% of training data as validation\n",
    "    verbose=1,                         # Show progress\n",
    "    callbacks=[early_stop]             # Early stopping callback\n",
    ")\n",
    "\n",
    "# Save final trained model\n",
    "model.save(\"CNN1D_final_model.keras\") # Save model for reuse\n",
    "print(\"Model saved as CNN1D_final_model.keras\")  # Print confirmation\n",
    "\n",
    "# Reuse saved model\n",
    "model_loaded = load_model(\"CNN1D_final_model.keras\")  # Load saved model\n",
    "scaler_loaded = pickle.load(open(\"scaler_X.pkl\", \"rb\"))  # Load saved scaler\n",
    "\n",
    "# Apply scaler and reshape data for prediction\n",
    "X_train_scaled_loaded = scaler_loaded.transform(X_train)[..., np.newaxis]  # Scale and reshape training data\n",
    "X_test_scaled_loaded = scaler_loaded.transform(X_test)[..., np.newaxis]    # Scale and reshape test data\n",
    "\n",
    "# Predict target values\n",
    "y_pred_train = model_loaded.predict(X_train_scaled_loaded).flatten()  # Training predictions\n",
    "y_pred_test = model_loaded.predict(X_test_scaled_loaded).flatten()    # Test predictions\n",
    "\n",
    "# Function to evaluate predictions\n",
    "def evaluate(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))  # Compute RMSE\n",
    "    r2 = r2_score(y_true, y_pred)                       # Compute R² score\n",
    "    rpd = np.std(y_true, ddof=1) / rmse                # Compute RPD (Ratio of Performance to Deviation)\n",
    "    return r2, rmse, rpd                                # Return metrics\n",
    "\n",
    "# Evaluate train and test predictions\n",
    "train_r2, train_rmse, train_rpd = evaluate(y_train, y_pred_train)  # Train metrics\n",
    "test_r2, test_rmse, test_rpd = evaluate(y_test, y_pred_test)       # Test metrics\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"\\nLoaded 1D-CNN Model Evaluation:\")\n",
    "print(f\"Train R²   : {train_r2:.4f}\")        # Print R² for training\n",
    "print(f\"Test R²    : {test_r2:.4f}\")         # Print R² for testing\n",
    "print(f\"Train RMSE : {train_rmse:.4f}\")      # Print RMSE for training\n",
    "print(f\"Test RMSE  : {test_rmse:.4f}\")       # Print RMSE for testing\n",
    "print(f\"Train RPD  : {train_rpd:.4f}\")       # Print RPD for training\n",
    "print(f\"Test RPD   : {test_rpd:.4f}\")        # Print RPD for testing\n",
    "\n",
    "# Create 2x2 grid plots for visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14,12))  # Create figure with 2x2 subplots\n",
    "\n",
    "# 1. Train scatter plot: predicted vs actual\n",
    "axes[0,0].scatter(y_train, y_pred_train, color='blue', alpha=0.6, label='Train Predicted')  # Scatter plot\n",
    "z_train = np.polyfit(y_train, y_pred_train, 1)      # Fit line for predictions\n",
    "p_train = np.poly1d(z_train)\n",
    "axes[0,0].plot(y_train, p_train(y_train), \"blue\", linestyle='--', label=\"Train Best Fit\")  # Plot best fit line\n",
    "axes[0,0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \"green\", linestyle='-', label=\"Ideal Fit\")  # Ideal y=x line\n",
    "axes[0,0].set_title(\"Training Set: Predicted vs Actual\")  # Set title\n",
    "axes[0,0].set_xlabel(\"Actual Values\")                     # X-axis label\n",
    "axes[0,0].set_ylabel(\"Predicted Values\")                  # Y-axis label\n",
    "axes[0,0].legend()                                        # Show legend\n",
    "axes[0,0].grid(True)                                      # Show grid\n",
    "\n",
    "# 2. Test scatter plot: predicted vs actual\n",
    "axes[0,1].scatter(y_test, y_pred_test, color='red', alpha=0.6, label='Test Predicted')      # Scatter plot\n",
    "z_test = np.polyfit(y_test, y_pred_test, 1)       # Fit line\n",
    "p_test = np.poly1d(z_test)\n",
    "axes[0,1].plot(y_test, p_test(y_test), \"orange\", linestyle='--', label=\"Test Best Fit\")    # Best fit line\n",
    "axes[0,1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"green\", linestyle='-', label=\"Ideal Fit\")  # Ideal line\n",
    "axes[0,1].set_title(\"Test Set: Predicted vs Actual\")  # Title\n",
    "axes[0,1].set_xlabel(\"Actual Values\")                 # X label\n",
    "axes[0,1].set_ylabel(\"Predicted Values\")              # Y label\n",
    "axes[0,1].legend()                                    # Legend\n",
    "axes[0,1].grid(True)                                  # Grid\n",
    "\n",
    "# 3. Training & validation loss over epochs\n",
    "axes[1,0].plot(history.history['loss'], label='Training Loss', color='blue')  # Training loss plot\n",
    "axes[1,0].plot(history.history['val_loss'], label='Validation Loss', color='orange')  # Validation loss\n",
    "axes[1,0].set_title(\"1D-CNN Training & Validation Loss\")   # Title\n",
    "axes[1,0].set_xlabel(\"Epochs\")                              # X-axis label\n",
    "axes[1,0].set_ylabel(\"MSE Loss\")                             # Y-axis label\n",
    "axes[1,0].legend()                                           # Legend\n",
    "axes[1,0].grid(True)                                         # Grid\n",
    "\n",
    "# 4. Confusion matrix for regression-as-classification\n",
    "y_test_class = (y_test >= np.median(y_test)).astype(int)          # Convert regression target to binary\n",
    "y_pred_test_class = (y_pred_test >= np.median(y_test)).astype(int)  # Convert predictions\n",
    "cm = confusion_matrix(y_test_class, y_pred_test_class)            # Compute confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Low Yield\", \"High Yield\"])  # Display\n",
    "disp.plot(ax=axes[1,1], cmap=plt.cm.Blues, values_format=\"d\")     # Plot confusion matrix\n",
    "axes[1,1].set_title(\"Confusion Matrix\")                            # Set title\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to avoid overlap\n",
    "plt.show()          # Show all plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55fbb41",
   "metadata": {},
   "source": [
    "Test Section"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
