{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79296312",
   "metadata": {},
   "source": [
    "Section 1(Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0baaa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max Normalization for Hyperspectral Data\n",
    "\n",
    "import pandas as pd                       # For reading/writing Excel/CSV and handling DataFrames\n",
    "from sklearn.preprocessing import MinMaxScaler   # For Min-Max scaling of features\n",
    "\n",
    "#  Load the dataset \n",
    "input_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/hyperspectral.xlsx\"  # Path to Excel file\n",
    "df = pd.read_excel(input_path)            # Read the Excel file into a DataFrame\n",
    "\n",
    "print(\"Original Columns:\", df.columns.tolist())   # Show all column names for verification\n",
    "print(\"Shape before:\", df.shape)                  # Print dataset shape (rows, columns) before preprocessing\n",
    "\n",
    "#  Drop unwanted columns \n",
    "drop_cols = ['Image ID', 'Minimum Reflectance', 'Otsu Threshold']  # Columns to remove\n",
    "df = df.drop(columns=drop_cols, errors='ignore')  # Drop them; ignore errors if any column not present\n",
    "\n",
    "#  Separate target column \n",
    "target_col = 'Yield'                      # This is the target variable\n",
    "y = df[target_col]                         # Store Yield separately (we don’t normalize it)\n",
    "X = df.drop(columns=[target_col])          # All other columns are features to normalize\n",
    "\n",
    "#  Min-Max normalization \n",
    "scaler = MinMaxScaler()                    # Create MinMaxScaler instance (default range: 0–1)\n",
    "X_scaled = scaler.fit_transform(X)         # Fit scaler to features & transform them into scaled array\n",
    "\n",
    "#  Reconstruct DataFrame \n",
    "normalized_df = pd.DataFrame(X_scaled, columns=X.columns)  # Convert scaled array back to DataFrame with original column names\n",
    "normalized_df[target_col] = y                               # Add back the original Yield column (unscaled)\n",
    "\n",
    "print(\"Shape after normalization:\", normalized_df.shape)    # Print new shape after dropping columns and adding scaled features\n",
    "\n",
    "#  Save processed file \n",
    "output_path = \"normalized_minmax_with_target.csv\"           # Output file name (will save in current working directory)\n",
    "normalized_df.to_csv(output_path, index=False)              # Save normalized data to CSV without row index\n",
    "\n",
    "print(f\"✅ Normalized file saved as: {output_path}\")         # Confirmation message after saving\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d4eb68",
   "metadata": {},
   "source": [
    "Section 2(KS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8b1b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data partition using KS(Kennard–Stone) method\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Load dataset\n",
    "input_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/normalized_minmax_with_target.csv\"\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Identify columns\n",
    "keep_cols = ['Image ID', 'Yield', 'Minimum Reflectance', 'Otsu']  # Columns not for splitting\n",
    "keep_cols_existing = [col for col in keep_cols if col in df.columns]  # Keep only existing ones\n",
    "X_cols = [col for col in df.columns if col not in keep_cols_existing]  # Band columns only\n",
    "X = df[X_cols].values  # Extract values for splitting\n",
    "\n",
    "# Define Kennard–Stone algorithm\n",
    "def kennard_stone(X, n_samples):\n",
    "    n_total = X.shape[0]\n",
    "    dist_matrix = cdist(X, X, metric='euclidean')  # Pairwise Euclidean distance\n",
    "    \n",
    "    # Step 1: select two farthest samples\n",
    "    i1, i2 = np.unravel_index(np.argmax(dist_matrix, axis=None), dist_matrix.shape)\n",
    "    selected = [i1, i2]\n",
    "    remaining = list(set(range(n_total)) - set(selected))\n",
    "    \n",
    "    # Step 2: iteratively select farthest points from selected set\n",
    "    while len(selected) < n_samples:\n",
    "        dist_to_selected = np.min(dist_matrix[remaining][:, selected], axis=1)\n",
    "        next_idx = remaining[np.argmax(dist_to_selected)]\n",
    "        selected.append(next_idx)\n",
    "        remaining.remove(next_idx)\n",
    "    \n",
    "    return selected\n",
    "\n",
    "# Split dataset\n",
    "n_train = int(0.8 * X.shape[0])  # 80% for training\n",
    "train_idx = kennard_stone(X, n_train)\n",
    "test_idx = list(set(range(X.shape[0])) - set(train_idx))\n",
    "\n",
    "df_train = df.iloc[train_idx].reset_index(drop=True)\n",
    "df_test = df.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "# Save output CSV files\n",
    "df_train.to_csv(r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/train_ks.csv\", index=False)\n",
    "df_test.to_csv(r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/test_ks.csv\", index=False)\n",
    "\n",
    "print(\"✅ Kennard–Stone split done!\")\n",
    "print(\"Train set shape:\", df_train.shape)\n",
    "print(\"Test set shape :\", df_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1bcf17",
   "metadata": {},
   "source": [
    "Section 3(PLSR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9c8405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using PLSR (Partial Least Squares Regression) technique\n",
    "import pandas as pd  # For data handling\n",
    "import numpy as np  # For numerical operations\n",
    "from sklearn.cross_decomposition import PLSRegression  # PLS regression model\n",
    "from sklearn.preprocessing import StandardScaler  # Feature scaling\n",
    "from sklearn.metrics import mean_squared_error, r2_score  # Evaluation metrics\n",
    "\n",
    "# Load train and test datasets\n",
    "train_df = pd.read_csv(r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/train_ks.csv\")  # Load training CSV\n",
    "test_df = pd.read_csv(r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/test_ks.csv\")  # Load testing CSV\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)  # Show training data shape\n",
    "print(\"Test shape:\", test_df.shape)  # Show testing data shape\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "target_column = \"Yield\"  # Replace with your actual target column name\n",
    "X_train = train_df.drop(columns=[target_column])  # Features for training\n",
    "y_train = train_df[target_column]  # Target for training\n",
    "\n",
    "X_test = test_df.drop(columns=[target_column])  # Features for testing\n",
    "y_test = test_df[target_column]  # Target for testing\n",
    "\n",
    "# Standardize the feature data\n",
    "scaler = StandardScaler()  # Initialize scaler\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit and transform training features\n",
    "X_test_scaled = scaler.transform(X_test)  # Transform testing features with same scaler\n",
    "\n",
    "# Train PLSR model with up to 10 components\n",
    "n_components = min(X_train.shape[1], 10)  # Choose number of components (max 10 or number of features)\n",
    "pls = PLSRegression(n_components=n_components)  # Initialize PLS model\n",
    "pls.fit(X_train_scaled, y_train)  # Fit PLS model to training data\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = pls.predict(X_train_scaled)  # Predict on training set\n",
    "y_pred_test = pls.predict(X_test_scaled)  # Predict on testing set\n",
    "\n",
    "# Evaluate model performance\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))  # RMSE for training\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))  # RMSE for testing\n",
    "\n",
    "train_r2 = r2_score(y_train, y_pred_train)  # R² for training\n",
    "test_r2 = r2_score(y_test, y_pred_test)  # R² for testing\n",
    "\n",
    "train_rpd = np.std(y_train, ddof=1) / train_rmse  # RPD for training\n",
    "test_rpd = np.std(y_test, ddof=1) / test_rmse  # RPD for testing\n",
    "\n",
    "print(\"\\nPLSR Model Evaluation:\")  # Print evaluation header\n",
    "print(f\"Train R²             : {train_r2:.4f}\")  # Display training R²\n",
    "print(f\"Test R²              : {test_r2:.4f}\")  # Display testing R²\n",
    "print(f\"Train RMSE           : {train_rmse:.4f}\")  # Display training RMSE\n",
    "print(f\"Test RMSE            : {test_rmse:.4f}\")  # Display testing RMSE\n",
    "print(f\"Train RPD            : {train_rpd:.4f}\")  # Display training RPD\n",
    "print(f\"Test RPD             : {test_rpd:.4f}\")  # Display testing RPD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52d9565",
   "metadata": {},
   "source": [
    "Section 4(RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51ca2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using RF (Random Forest) model\n",
    "import pandas as pd  # For data handling\n",
    "import numpy as np  # For numerical operations\n",
    "from sklearn.ensemble import RandomForestRegressor  # Random Forest Regressor\n",
    "from sklearn.model_selection import train_test_split  # Train/test split if needed\n",
    "from sklearn.preprocessing import StandardScaler  # Feature scaling\n",
    "from sklearn.metrics import mean_squared_error, r2_score  # Evaluation metrics\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/train_ks.csv\")  # Load training CSV\n",
    "test_df = pd.read_csv(r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/test_ks.csv\")  # Load testing CSV\n",
    "\n",
    "target_column = \"Yield\"  # Define target column\n",
    "X_train = train_df.drop(columns=[target_column]).values  # Features for training\n",
    "y_train = train_df[target_column].values  # Target for training\n",
    "\n",
    "X_test = test_df.drop(columns=[target_column]).values  # Features for testing\n",
    "y_test = test_df[target_column].values  # Target for testing\n",
    "\n",
    "# Standardize features (optional but recommended for some models)\n",
    "scaler = StandardScaler()  # Initialize scaler\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit and transform training features\n",
    "X_test_scaled = scaler.transform(X_test)  # Transform testing features with same scaler\n",
    "\n",
    "# CPU Random Forest Model\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=500,  # Number of trees\n",
    "    max_depth=30,  # Maximum depth of each tree\n",
    "    max_features='sqrt',  # Max features considered at each split\n",
    "    random_state=42,  # Ensure reproducibility\n",
    "    n_jobs=-1  # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train_scaled, y_train)  # Fit model to training data\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = rf.predict(X_train_scaled)  # Predict on training set\n",
    "y_pred_test = rf.predict(X_test_scaled)  # Predict on testing set\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))  # Compute RMSE\n",
    "    r2 = r2_score(y_true, y_pred)  # Compute R² score\n",
    "    rpd = np.std(y_true, ddof=1) / rmse  # Compute RPD\n",
    "    return r2, rmse, rpd  # Return evaluation metrics\n",
    "\n",
    "train_r2, train_rmse, train_rpd = evaluate(y_train, y_pred_train)  # Evaluate training performance\n",
    "test_r2, test_rmse, test_rpd = evaluate(y_test, y_pred_test)  # Evaluate testing performance\n",
    "\n",
    "print(\"\\nRandom Forest Model Evaluation:\")  # Print evaluation header\n",
    "print(f\"Train R²   : {train_r2:.4f}\")  # Display training R²\n",
    "print(f\"Test R²    : {test_r2:.4f}\")  # Display testing R²\n",
    "print(f\"Train RMSE : {train_rmse:.4f}\")  # Display training RMSE\n",
    "print(f\"Test RMSE  : {test_rmse:.4f}\")  # Display testing RMSE\n",
    "print(f\"Train RPD  : {train_rpd:.4f}\")  # Display training RPD\n",
    "print(f\"Test RPD   : {test_rpd:.4f}\")  # Display testing RPD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb717cdb",
   "metadata": {},
   "source": [
    "Section 5(CNN-1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccceb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN-1D (Convolutional Neural Network – One Dimensional)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pickle\n",
    "\n",
    "# Load CSV files\n",
    "train_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/train_ks.csv\"\n",
    "test_path = r\"D:/Z Education/University/4-1 Course/Thesis/Hyper leaf dataset/test_ks.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Features and target\n",
    "target_column = \"Yield\"\n",
    "X_train = train_df.drop(columns=[target_column]).values\n",
    "y_train = train_df[target_column].values\n",
    "X_test = test_df.drop(columns=[target_column]).values\n",
    "y_test = test_df[target_column].values\n",
    "\n",
    "# Standardize features\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "# Save scaler for reuse\n",
    "with open(\"scaler_X.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler_X, f)\n",
    "\n",
    "# Reshape for 1D-CNN input\n",
    "X_train_scaled = X_train_scaled[..., np.newaxis]\n",
    "X_test_scaled = X_test_scaled[..., np.newaxis]\n",
    "\n",
    "# Build and train 1D-CNN model\n",
    "model = Sequential([\n",
    "    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train_scaled.shape[1],1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Conv1D(128, kernel_size=3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# Save final model\n",
    "model.save(\"CNN1D_final_model.keras\")\n",
    "print(\"Model saved as CNN1D_final_model.keras\")\n",
    "\n",
    "# REUSE SAVED MODEL \n",
    "model_loaded = load_model(\"CNN1D_final_model.keras\")\n",
    "scaler_loaded = pickle.load(open(\"scaler_X.pkl\", \"rb\"))\n",
    "\n",
    "X_train_scaled_loaded = scaler_loaded.transform(X_train)[..., np.newaxis]\n",
    "X_test_scaled_loaded = scaler_loaded.transform(X_test)[..., np.newaxis]\n",
    "\n",
    "y_pred_train = model_loaded.predict(X_train_scaled_loaded).flatten()\n",
    "y_pred_test = model_loaded.predict(X_test_scaled_loaded).flatten()\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rpd = np.std(y_true, ddof=1) / rmse\n",
    "    return r2, rmse, rpd\n",
    "\n",
    "train_r2, train_rmse, train_rpd = evaluate(y_train, y_pred_train)\n",
    "test_r2, test_rmse, test_rpd = evaluate(y_test, y_pred_test)\n",
    "\n",
    "print(\"\\nLoaded 1D-CNN Model Evaluation:\")\n",
    "print(f\"Train R²   : {train_r2:.4f}\")\n",
    "print(f\"Test R²    : {test_r2:.4f}\")\n",
    "print(f\"Train RMSE : {train_rmse:.4f}\")\n",
    "print(f\"Test RMSE  : {test_rmse:.4f}\")\n",
    "print(f\"Train RPD  : {train_rpd:.4f}\")\n",
    "print(f\"Test RPD   : {test_rpd:.4f}\")\n",
    "\n",
    "# 2x2 Grid Plot\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14,12))\n",
    "\n",
    "# 1. Train scatter\n",
    "axes[0,0].scatter(y_train, y_pred_train, color='blue', alpha=0.6, label='Train Predicted')\n",
    "z_train = np.polyfit(y_train, y_pred_train, 1)\n",
    "p_train = np.poly1d(z_train)\n",
    "axes[0,0].plot(y_train, p_train(y_train), \"blue\", linestyle='--', label=\"Train Best Fit\")\n",
    "axes[0,0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \"green\", linestyle='-', label=\"Ideal Fit\")\n",
    "axes[0,0].set_title(\"Training Set: Predicted vs Actual\")\n",
    "axes[0,0].set_xlabel(\"Actual Values\")\n",
    "axes[0,0].set_ylabel(\"Predicted Values\")\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True)\n",
    "\n",
    "# 2. Test scatter\n",
    "axes[0,1].scatter(y_test, y_pred_test, color='red', alpha=0.6, label='Test Predicted')\n",
    "z_test = np.polyfit(y_test, y_pred_test, 1)\n",
    "p_test = np.poly1d(z_test)\n",
    "axes[0,1].plot(y_test, p_test(y_test), \"orange\", linestyle='--', label=\"Test Best Fit\")\n",
    "axes[0,1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"green\", linestyle='-', label=\"Ideal Fit\")\n",
    "axes[0,1].set_title(\"Test Set: Predicted vs Actual\")\n",
    "axes[0,1].set_xlabel(\"Actual Values\")\n",
    "axes[0,1].set_ylabel(\"Predicted Values\")\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True)\n",
    "\n",
    "# 3. Training & Validation Loss\n",
    "axes[1,0].plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "axes[1,0].plot(history.history['val_loss'], label='Validation Loss', color='orange')\n",
    "axes[1,0].set_title(\"1D-CNN Training & Validation Loss\")\n",
    "axes[1,0].set_xlabel(\"Epochs\")\n",
    "axes[1,0].set_ylabel(\"MSE Loss\")\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True)\n",
    "\n",
    "# 4. Confusion Matrix (Regression → Classification)\n",
    "y_test_class = (y_test >= np.median(y_test)).astype(int)\n",
    "y_pred_test_class = (y_pred_test >= np.median(y_test)).astype(int)\n",
    "cm = confusion_matrix(y_test_class, y_pred_test_class)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Low Yield\", \"High Yield\"])\n",
    "disp.plot(ax=axes[1,1], cmap=plt.cm.Blues, values_format=\"d\")\n",
    "axes[1,1].set_title(\"Confusion Matrix\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55fbb41",
   "metadata": {},
   "source": [
    "Test Section"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
